{"cells":[{"cell_type":"markdown","metadata":{"id":"PlpG_fIiqdJU","colab_type":"text","cell_id":"172d512b144644108e5716837b9556c4","deepnote_cell_type":"markdown"},"source":"# Neural Network Exercise\n\nIn this Exercise Notebook you will be building your own artificial neural network and seeing how adding different types of layers can affect the validation/testing accuracy. This is based off of the Simple Neural Network with Keras tutorial, so you can reference that for further explanations as well."},{"cell_type":"code","metadata":{"id":"eFSzOvSduDf8","colab":{},"colab_type":"code","source_hash":"9e6326f3","execution_start":1689868787438,"execution_millis":95,"deepnote_to_be_reexecuted":false,"cell_id":"7564357a0cb44742bcb50c027adcba6b","deepnote_cell_type":"code"},"source":"import os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf","execution_count":120,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ep_09mlyqa5G","colab":{},"colab_type":"code","source_hash":"7a6e8b1d","execution_start":1689868787453,"execution_millis":202,"deepnote_to_be_reexecuted":false,"cell_id":"4e62d0e18b3541038f0f586f1f02bf8f","deepnote_cell_type":"code"},"source":"os.system('wget https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/spoken_digit_manual_features.csv')","execution_count":121,"outputs":[{"name":"stderr","text":"--2023-07-20 15:59:47--  https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/spoken_digit_manual_features.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 220478 (215K) [text/plain]\nSaving to: ‘spoken_digit_manual_features.csv.8’\n\n     0K .......... .......... .......... .......... .......... 23%  168M 0s\n    50K .......... .......... .......... .......... .......... 46% 47.2M 0s\n   100K .......... .......... .......... .......... .......... 69% 45.6M 0s\n   150K .......... .......... .......... .......... .......... 92%  197M 0s\n   200K .......... .....                                      100%  372M=0.003s\n\n2023-07-20 15:59:47 (78.3 MB/s) - ‘spoken_digit_manual_features.csv.8’ saved [220478/220478]\n\n","output_type":"stream"},{"output_type":"execute_result","execution_count":121,"data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"zChcGaqVysRB","colab_type":"text","cell_id":"c34f64f8ea674fee9c6f0ee8906606ad","deepnote_cell_type":"markdown"},"source":"## Load Training Data and Pre-processed Features\n\nYour goal is to build a neural network that learns to classify which of the 5 speakers is recorded in a signal sample. Your prediction will be based off of features we've already pre-extracted for you and put into this CSV: spectral centroid `SC`, spectral flatness `SF`, and maximum frequency `MF`."},{"cell_type":"code","metadata":{"id":"SVwsmOGvw7jp","colab":{},"colab_type":"code","source_hash":"acd4a82d","execution_start":1689868787530,"execution_millis":151,"deepnote_to_be_reexecuted":false,"cell_id":"d27f7479a0144977afba955c438a948e","deepnote_cell_type":"code"},"source":"# Load csv containing raw data, labels, and pre-processed features\nspoken_df = pd.read_csv('spoken_digit_manual_features.csv', index_col = 0)\nprint(spoken_df.head(10))\nprint('\\n')\n\n# Set speakers\nspeakers = set(spoken_df['speaker'])\nprint(f'There are {len(speakers)} unique speakers in the dataset')","execution_count":122,"outputs":[{"name":"stdout","text":"                file  digit   speaker  trial           SC        SF  \\\n0   5_yweweler_8.wav      5  yweweler      8  1029.497959  0.397336   \n1    3_george_49.wav      3    george      4  1881.296834  0.387050   \n2  9_yweweler_44.wav      9  yweweler      4  1093.951856  0.394981   \n3  8_yweweler_33.wav      8  yweweler      3  1409.543285  0.487496   \n4      7_theo_34.wav      7      theo      3   887.361601  0.396825   \n5   1_jackson_45.wav      1   jackson      4  1007.568129  0.324100   \n6  6_yweweler_18.wav      6  yweweler      1  1286.701352  0.498813   \n7    9_george_35.wav      9    george      3  1405.092061  0.353083   \n8   9_jackson_32.wav      9   jackson      3  1172.899961  0.477907   \n9    8_george_26.wav      8    george      2  1959.977577  0.462901   \n\n           MF  \n0  745.878340  \n1  323.943662  \n2  244.648318  \n3  392.350401  \n4  130.640309  \n5  216.306156  \n6  400.715564  \n7  447.239693  \n8  114.892780  \n9  320.537966  \n\n\nThere are 5 unique speakers in the dataset\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"mArY7lB4Akv1","colab_type":"text","cell_id":"a8b938021f2b437994bd5f9609a58c15","deepnote_cell_type":"markdown"},"source":"Converting labels to \"onehot\" vectors:"},{"cell_type":"code","metadata":{"id":"nLRtFkiYAc3N","colab":{},"colab_type":"code","source_hash":"a29d6318","execution_start":1689868787537,"execution_millis":17,"deepnote_to_be_reexecuted":false,"cell_id":"29b15c4b547440ec80e3bcaf0ef6dde1","deepnote_cell_type":"code"},"source":"# Make dictionary to convert from speaker names to indices\nname2int_dict = {name: ind for (ind, name) in enumerate(set(spoken_df['speaker']))}\n\ny_labels = spoken_df['speaker']\n# Set y_labels to be indices of speaker\ny_labels = [name2int_dict[name] for name in y_labels]","execution_count":123,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xUhDZMw1A93D","colab_type":"text","cell_id":"49baeee424d94f1fbc3043f85774cebc","deepnote_cell_type":"markdown"},"source":"Standardize data and split into train, validation, and test sets:"},{"cell_type":"code","metadata":{"id":"TAuzw6ibA3Es","colab":{},"colab_type":"code","source_hash":"524f00d8","execution_start":1689868787542,"execution_millis":70,"deepnote_to_be_reexecuted":false,"cell_id":"689b5077f02b421b8970e7cfa304e465","deepnote_cell_type":"code"},"source":"# Downselect to only the 3 columns of the dataset we are learning from, aka the features\nX_data = spoken_df[['SC', 'SF', 'MF']].to_numpy()\n\n# Decide how large to make validation and test sets\nn_val = 250\nn_test = 250\n\n# Shuffle data before partitioning\nX_data, y_labels = shuffle(X_data, y_labels, random_state = 25)\n\n# Partition\nX_data_test, y_labels_test = X_data[:n_test,:], y_labels[:n_test]\nX_data_val, y_labels_val = X_data[n_test:n_test+n_val,:], y_labels[n_test:n_test+n_val]\nX_data_train, y_labels_train = X_data[n_test+n_val:,:], y_labels[n_test+n_val:]\n\n# Scale data\nscaler = StandardScaler()\nX_data_train=scaler.fit_transform(X_data_train)\nX_data_val = scaler.transform(X_data_val)\nX_data_test = scaler.transform(X_data_test)\n\n# Convert labels to onehot\ny_labels_train = tf.keras.utils.to_categorical(y_labels_train, 5)\ny_labels_val =  tf.keras.utils.to_categorical(y_labels_val, 5)\ny_labels_test =  tf.keras.utils.to_categorical(y_labels_test, 5)\n\ntraining_set = tf.data.Dataset.from_tensor_slices((X_data_train, y_labels_train))","execution_count":124,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"riycN8SdyxNT","colab_type":"text","cell_id":"6f40c6b0a4734c77b6b55183a848322c","deepnote_cell_type":"markdown"},"source":"## Aditional Layers\n\nBefore you get to writing your own neural network we'll show you some examples of additional layers you can potetially add that we didn't go over in the tutorial. After reading over our explanations/example code and going through documentation you'll be testing some of these out by putting together a neural network yourself."},{"cell_type":"markdown","metadata":{"id":"DoaZsqc3iEyv","colab_type":"text","cell_id":"216571f34abc440aa045975ec8d04e12","deepnote_cell_type":"markdown"},"source":"### Dropout Layers\n\nDropout layers randomly omit, or drop, some elements of the output vector from the layer, which helps prevent overfitting and can improve the generalization of your neural network. The dropout rate can be any number between 0 and 1.\n\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n\n```python\n# Example\nd_r = 0.6\ntf.keras.layers.Dropout(rate=d_r)\n```"},{"cell_type":"markdown","metadata":{"id":"gVJMrZ09iUgk","colab_type":"text","cell_id":"d89eba378f704f6c85b309101294a951","deepnote_cell_type":"markdown"},"source":"### Pooling Layers\n\nA pooling layer reduces dimensionality (reducing the size of each feature map) and \"compresses\" information by combining several output elements. Two common functions used for pooling are:\n- Average pooling: calculating the average value for each patch on the feature map\n- Max pooling: calculating the maximum value for each patch of the feature map\n\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D\n\n```python\n# Example\ntf.keras.layers.MaxPool1D(pool_size=1)\n```"},{"cell_type":"markdown","metadata":{"id":"72ngF_beiaV9","colab_type":"text","cell_id":"7f130fde24114e5ba09803a7da8e1399","deepnote_cell_type":"markdown"},"source":"### Activation Layers/Functions\n\nAn activation function looks at each \"neuron\" in your neural network and determines whether it should be activated (fired) or not, based on the relevancy of the neuron's input to the model’s predictions. Some different activation functions you could look at are:\n- softmax https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax\n- sigmoid https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid\n- softplus https://www.tensorflow.org/api_docs/python/tf/keras/activations/softplus\n- relu https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU\n\n```python\n# Example\ntf.keras.layers.Softmax()\n```"},{"cell_type":"markdown","metadata":{"id":"zCdxM6HDqR1F","colab_type":"text","cell_id":"33dc56be424c4b4cb1f1eb7de466cc8f","deepnote_cell_type":"markdown"},"source":"### Optimation Functions\n\nOptimation functions\n- Adam https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n  - Adam is computationally efficient, has little memory requirement, and is well suited for problems that are large in terms of data/parameter.\n- Adagrad https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad\n  - Adagrad is an optimizer that is best used for sparse data. Some of its benefits are that it converges more quickly and doesn't need manual adjustment of the hyperparameter \"learning rate\".\n- SGD https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD\n  - SGD is a stochastic gradient descent and momentum optimizer. SGD essentially helps gradient vectors move down loss functions towards the minimum point, leading to faster \"converging\".\n- RMSprop https://keras.io/api/optimizers/rmsprop/\n  - As you may already know, the learning rate regulates how much the model \ncan change based on the estimated error (which occurs every time the model's weights are updated). Instead of treating the learning rate as a hyperparamter, RMSprop is an optimization technique that uses relies on a changing, adaptive learning rate.\n\n```python\n# Example code\nl_r = .001 \ntf.keras.optimizers.SGD(learning_rate=l_r)\n```"},{"cell_type":"markdown","metadata":{"id":"ldbularZ3cCW","colab_type":"text","cell_id":"b200d87386d149ccb935392530287ce6","deepnote_cell_type":"markdown"},"source":"## Putting Together Your Neural Network\n\nNow you will experiment with adding different layers to your neural network. We've added some guiding comments to give you a place to start and test out, but we also strongly encourage you to go through all the documetation and do some googling as well!"},{"cell_type":"code","metadata":{"id":"qMp_z7W9vZV4","colab":{},"colab_type":"code","source_hash":"da8d0ff2","execution_start":1689868787566,"execution_millis":56,"deepnote_to_be_reexecuted":false,"cell_id":"7b180f3740cf494b96f4745203550143","deepnote_cell_type":"code"},"source":"# Once you've gone through all the tests play around with these rate alues to see if you can increase your accuracy\nl_r = .001 \nd_r = 0.6\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(8, input_shape=(3,)))","execution_count":125,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8OQKRfNjBWGC","colab_type":"text","cell_id":"0a6dd5f8bf4a46849b2fff3f05170327","deepnote_cell_type":"markdown"},"source":"### Test 1"},{"cell_type":"code","metadata":{"id":"BneaEDk-BWj2","colab":{},"colab_type":"code","source_hash":"c81d8c48","execution_start":1689868787571,"execution_millis":64,"deepnote_to_be_reexecuted":false,"cell_id":"044dbf52e78e463dae603007ba836832","deepnote_cell_type":"code"},"source":"# Run this cell as it is\nmodel.add(tf.keras.layers.Dense(8))\nmodel.add(tf.keras.layers.Dense(8))\n\n# output dimension needs to be number of classes in order for each to get a score\nmodel.add(tf.keras.layers.Dense(5))\n\n# Now skip down to the section that compiles and trains your model and run those cells.\n# Check the pseudo-test accuracy and see how well the bare minimum performed.\n\n# loss: 4.1262 - accuracy: 0.1760","execution_count":126,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djwaQho7_xBt","colab_type":"text","cell_id":"4e0966e686f54c3688e9b965cc40c8e7","deepnote_cell_type":"markdown"},"source":"### Test 2"},{"cell_type":"code","metadata":{"id":"o2MQzNEa_ViW","colab":{},"colab_type":"code","source_hash":"9a6b053f","execution_start":1689868787601,"execution_millis":60,"deepnote_to_be_reexecuted":false,"cell_id":"663164b6e3074d9c8cb6fca49991f2d8","deepnote_cell_type":"code"},"source":"# Add activation layer here\nmodel.add(tf.keras.layers.Dense(8))\n# Add activation layer here\nmodel.add(tf.keras.layers.Dense(8))\n# Add activation layer here \nmodel.add(tf.keras.layers.Dense(8))\n\n# output dimension needs to be number of classes in order for each to get a score\nmodel.add(tf.keras.layers.Dense(5))\n\n# Now skip down to the section that compiles and trains your model and re-run those cells.\n# What do you notice about the testing/Validation accuracy after Test 2 in comparison to Test 1?\n# loss: 2.2187 - accuracy: 0.1920","execution_count":127,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_pr4L_0Bzt_","colab_type":"text","cell_id":"7867fd42359c4e6eb3b0803720b35b69","deepnote_cell_type":"markdown"},"source":"### Test 3"},{"cell_type":"code","metadata":{"id":"pb-7sVYkB11c","colab":{},"colab_type":"code","source_hash":"78dbf38f","execution_start":1689868787692,"execution_millis":7,"deepnote_to_be_reexecuted":false,"cell_id":"4c4358d075f5406baf2128c2cf44e916","deepnote_cell_type":"code"},"source":"# Add activation layer here\nmodel.add(tf.keras.layers.Dense(8))\n# Add activation layer here\nmodel.add(tf.keras.layers.Dense(8))\n# Add activation layer here \nmodel.add(tf.keras.layers.Dense(8))\n\n# output dimension needs to be number of classes in order for each to get a score\nmodel.add(tf.keras.layers.Dense(5))\n\n# Add dropout layer here\nd_r = 0.6\ntf.keras.layers.Dropout(rate=d_r)\n\n# Now skip down to the section that compiles and trains your model and re-run those cells.\n# What do you notice about the testing/Validation accuracy after Test 2 in comparison to Test 1 & 2?\n# loss: 3.0384 - accuracy: 0.2360","execution_count":128,"outputs":[{"output_type":"execute_result","execution_count":128,"data":{"text/plain":"<keras.layers.regularization.dropout.Dropout at 0x7f28147386a0>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"YwjD-MgMDHFV","colab_type":"text","cell_id":"2af7cf7084094b5faa4a4ef13cb8ff62","deepnote_cell_type":"markdown"},"source":"### Test 4\n\nNow go back down to the cell where you compiled your model, and this time change the optimizer. It's been set to Adam by default but as we showed you above, there are other functions that you can test out."},{"cell_type":"markdown","metadata":{"id":"Qhna7lr5Dm56","colab_type":"text","cell_id":"7e14073642a84a40ab871cd49e79396d","deepnote_cell_type":"markdown"},"source":"## Compiling and Training Your Model"},{"cell_type":"code","metadata":{"id":"XB1qNpsoASGv","colab":{},"colab_type":"code","source_hash":"ad2d3531","execution_start":1689868787694,"execution_millis":22,"deepnote_to_be_reexecuted":false,"cell_id":"381997f1b6244a029a6153b835ff1b80","deepnote_cell_type":"code"},"source":"model.compile(loss = tf.keras.losses.categorical_crossentropy, \n              optimizer = tf.keras.optimizers.Adam(learning_rate=l_r),\n              metrics = ['accuracy'])   ","execution_count":129,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tj1lwAY7BJPk","colab_type":"text","cell_id":"9f5f263b02984dd88a18e1ebc68ee18b","deepnote_cell_type":"markdown"},"source":"Fit Model to Data, Specify Number of Epochs and Batch Size:"},{"cell_type":"code","metadata":{"id":"uF2RT0eGBIlD","colab":{},"colab_type":"code","source_hash":"4ac3584e","execution_start":1689868787695,"execution_millis":9511,"deepnote_to_be_reexecuted":false,"cell_id":"2a3ea1105db2447dafeea71c62aff272","deepnote_cell_type":"code"},"source":"EPOCHS = 50\nbatch_size = 100\n\nbatch = training_set.batch(batch_size) #set batch size\n\nfor epoch in range(EPOCHS):\n    for signals, labels in batch:\n        tr_loss, tr_accuracy = model.train_on_batch(signals, labels)\n    val_loss, val_accuracy = model.evaluate(X_data_val, y_labels_val)\n    print(('Epoch #%d\\t Training Loss: %.2f\\tTraining Accuracy: %.2f\\t'\n         'Validation Loss: %.2f\\tValidation Accuracy: %.2f')\n         % (epoch + 1, tr_loss, tr_accuracy,\n         val_loss, val_accuracy))","execution_count":130,"outputs":[{"name":"stdout","text":"8/8 [==============================] - 0s 1ms/step - loss: 6.7427 - accuracy: 0.1440\n2023-07-20 15:59:48.316861: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\nEpoch #1\t Training Loss: 6.46\tTraining Accuracy: 0.16\tValidation Loss: 6.74\tValidation Accuracy: 0.14\n8/8 [==============================] - 0s 958us/step - loss: 6.7722 - accuracy: 0.1480\nEpoch #2\t Training Loss: 6.49\tTraining Accuracy: 0.14\tValidation Loss: 6.77\tValidation Accuracy: 0.15\n8/8 [==============================] - 0s 1ms/step - loss: 6.8220 - accuracy: 0.1520\nEpoch #3\t Training Loss: 5.76\tTraining Accuracy: 0.14\tValidation Loss: 6.82\tValidation Accuracy: 0.15\n8/8 [==============================] - 0s 1ms/step - loss: 6.4516 - accuracy: 0.1560\nEpoch #4\t Training Loss: 4.80\tTraining Accuracy: 0.15\tValidation Loss: 6.45\tValidation Accuracy: 0.16\n8/8 [==============================] - 0s 2ms/step - loss: 6.4500 - accuracy: 0.1520\n2023-07-20 15:59:49.086170: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\nEpoch #5\t Training Loss: 5.27\tTraining Accuracy: 0.15\tValidation Loss: 6.45\tValidation Accuracy: 0.15\n8/8 [==============================] - 0s 1ms/step - loss: 6.6630 - accuracy: 0.1480\nEpoch #6\t Training Loss: 5.40\tTraining Accuracy: 0.15\tValidation Loss: 6.66\tValidation Accuracy: 0.15\n8/8 [==============================] - 0s 1ms/step - loss: 7.0857 - accuracy: 0.1640\nEpoch #7\t Training Loss: 6.10\tTraining Accuracy: 0.16\tValidation Loss: 7.09\tValidation Accuracy: 0.16\n8/8 [==============================] - 0s 1ms/step - loss: 7.0014 - accuracy: 0.1680\nEpoch #8\t Training Loss: 7.66\tTraining Accuracy: 0.12\tValidation Loss: 7.00\tValidation Accuracy: 0.17\n2023-07-20 15:59:49.529329: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 1ms/step - loss: 7.3295 - accuracy: 0.2160\nEpoch #9\t Training Loss: 7.04\tTraining Accuracy: 0.16\tValidation Loss: 7.33\tValidation Accuracy: 0.22\n8/8 [==============================] - 0s 1ms/step - loss: 6.7906 - accuracy: 0.2560\nEpoch #10\t Training Loss: 6.88\tTraining Accuracy: 0.21\tValidation Loss: 6.79\tValidation Accuracy: 0.26\n8/8 [==============================] - 0s 1ms/step - loss: 6.7073 - accuracy: 0.2440\nEpoch #11\t Training Loss: 5.88\tTraining Accuracy: 0.21\tValidation Loss: 6.71\tValidation Accuracy: 0.24\n8/8 [==============================] - 0s 997us/step - loss: 6.3170 - accuracy: 0.1960\nEpoch #12\t Training Loss: 5.21\tTraining Accuracy: 0.20\tValidation Loss: 6.32\tValidation Accuracy: 0.20\n8/8 [==============================] - 0s 1ms/step - loss: 5.9187 - accuracy: 0.2000\nEpoch #13\t Training Loss: 4.92\tTraining Accuracy: 0.18\tValidation Loss: 5.92\tValidation Accuracy: 0.20\n8/8 [==============================] - 0s 3ms/step - loss: 6.2887 - accuracy: 0.1800\nEpoch #14\t Training Loss: 6.92\tTraining Accuracy: 0.15\tValidation Loss: 6.29\tValidation Accuracy: 0.18\n2023-07-20 15:59:50.589305: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n2023-07-20 15:59:50.599177: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 1ms/step - loss: 6.1667 - accuracy: 0.1680\nEpoch #15\t Training Loss: 6.97\tTraining Accuracy: 0.17\tValidation Loss: 6.17\tValidation Accuracy: 0.17\n8/8 [==============================] - 0s 994us/step - loss: 5.7081 - accuracy: 0.1840\nEpoch #16\t Training Loss: 6.41\tTraining Accuracy: 0.18\tValidation Loss: 5.71\tValidation Accuracy: 0.18\n8/8 [==============================] - 0s 970us/step - loss: 5.4675 - accuracy: 0.1840\nEpoch #17\t Training Loss: 5.53\tTraining Accuracy: 0.18\tValidation Loss: 5.47\tValidation Accuracy: 0.18\n8/8 [==============================] - 0s 1ms/step - loss: 5.1488 - accuracy: 0.1840\nEpoch #18\t Training Loss: 5.02\tTraining Accuracy: 0.18\tValidation Loss: 5.15\tValidation Accuracy: 0.18\n8/8 [==============================] - 0s 2ms/step - loss: 3.8481 - accuracy: 0.1880\nEpoch #19\t Training Loss: 4.39\tTraining Accuracy: 0.21\tValidation Loss: 3.85\tValidation Accuracy: 0.19\n1/8 [==>...........................] - ETA: 0s - loss: 4.9558 - accuracy: 0.21882023-07-20 15:59:51.555441: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 1ms/step - loss: 3.8799 - accuracy: 0.1880\nEpoch #20\t Training Loss: 4.04\tTraining Accuracy: 0.21\tValidation Loss: 3.88\tValidation Accuracy: 0.19\n8/8 [==============================] - 0s 961us/step - loss: 3.7733 - accuracy: 0.1640\nEpoch #21\t Training Loss: 3.60\tTraining Accuracy: 0.19\tValidation Loss: 3.77\tValidation Accuracy: 0.16\n2023-07-20 15:59:51.801139: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 1ms/step - loss: 3.7943 - accuracy: 0.1640\nEpoch #22\t Training Loss: 4.04\tTraining Accuracy: 0.17\tValidation Loss: 3.79\tValidation Accuracy: 0.16\n2023-07-20 15:59:52.023006: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n2023-07-20 15:59:52.152139: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n2023-07-20 15:59:52.220200: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 976us/step - loss: 3.8475 - accuracy: 0.1760\nEpoch #23\t Training Loss: 4.14\tTraining Accuracy: 0.18\tValidation Loss: 3.85\tValidation Accuracy: 0.18\n8/8 [==============================] - 0s 1ms/step - loss: 3.7211 - accuracy: 0.1800\nEpoch #24\t Training Loss: 4.26\tTraining Accuracy: 0.16\tValidation Loss: 3.72\tValidation Accuracy: 0.18\n2023-07-20 15:59:52.316253: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 995us/step - loss: 3.8023 - accuracy: 0.1880\nEpoch #25\t Training Loss: 4.02\tTraining Accuracy: 0.16\tValidation Loss: 3.80\tValidation Accuracy: 0.19\n8/8 [==============================] - 0s 960us/step - loss: 4.1629 - accuracy: 0.2040\nEpoch #26\t Training Loss: 4.41\tTraining Accuracy: 0.15\tValidation Loss: 4.16\tValidation Accuracy: 0.20\n8/8 [==============================] - 0s 1ms/step - loss: 4.5389 - accuracy: 0.2040\nEpoch #27\t Training Loss: 4.98\tTraining Accuracy: 0.14\tValidation Loss: 4.54\tValidation Accuracy: 0.20\n2023-07-20 15:59:52.908844: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 1ms/step - loss: 5.2632 - accuracy: 0.2040\nEpoch #28\t Training Loss: 5.93\tTraining Accuracy: 0.14\tValidation Loss: 5.26\tValidation Accuracy: 0.20\n8/8 [==============================] - 0s 989us/step - loss: 6.8159 - accuracy: 0.2160\nEpoch #29\t Training Loss: 7.24\tTraining Accuracy: 0.15\tValidation Loss: 6.82\tValidation Accuracy: 0.22\n2023-07-20 15:59:53.218318: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 1ms/step - loss: 6.8854 - accuracy: 0.1520\nEpoch #30\t Training Loss: 8.11\tTraining Accuracy: 0.20\tValidation Loss: 6.89\tValidation Accuracy: 0.15\n8/8 [==============================] - 0s 1ms/step - loss: 7.0148 - accuracy: 0.1520\nEpoch #31\t Training Loss: 7.88\tTraining Accuracy: 0.20\tValidation Loss: 7.01\tValidation Accuracy: 0.15\n8/8 [==============================] - 0s 977us/step - loss: 6.9490 - accuracy: 0.1520\nEpoch #32\t Training Loss: 7.82\tTraining Accuracy: 0.20\tValidation Loss: 6.95\tValidation Accuracy: 0.15\n8/8 [==============================] - 0s 937us/step - loss: 6.9168 - accuracy: 0.1480\nEpoch #33\t Training Loss: 7.96\tTraining Accuracy: 0.20\tValidation Loss: 6.92\tValidation Accuracy: 0.15\n8/8 [==============================] - 0s 1ms/step - loss: 6.9509 - accuracy: 0.1600\nEpoch #34\t Training Loss: 7.66\tTraining Accuracy: 0.20\tValidation Loss: 6.95\tValidation Accuracy: 0.16\n8/8 [==============================] - 0s 1ms/step - loss: 6.9639 - accuracy: 0.1600\nEpoch #35\t Training Loss: 7.88\tTraining Accuracy: 0.21\tValidation Loss: 6.96\tValidation Accuracy: 0.16\n2023-07-20 15:59:54.223497: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 1ms/step - loss: 6.9936 - accuracy: 0.1600\nEpoch #36\t Training Loss: 7.71\tTraining Accuracy: 0.21\tValidation Loss: 6.99\tValidation Accuracy: 0.16\n8/8 [==============================] - 0s 1ms/step - loss: 6.6300 - accuracy: 0.1640\nEpoch #37\t Training Loss: 7.39\tTraining Accuracy: 0.21\tValidation Loss: 6.63\tValidation Accuracy: 0.16\n8/8 [==============================] - 0s 1ms/step - loss: 6.5818 - accuracy: 0.1600\nEpoch #38\t Training Loss: 7.30\tTraining Accuracy: 0.19\tValidation Loss: 6.58\tValidation Accuracy: 0.16\n2023-07-20 15:59:54.751456: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n2023-07-20 15:59:54.811185: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 2ms/step - loss: 6.5177 - accuracy: 0.1840\n2023-07-20 15:59:54.964210: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n2023-07-20 15:59:55.159102: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\nEpoch #39\t Training Loss: 7.20\tTraining Accuracy: 0.18\tValidation Loss: 6.52\tValidation Accuracy: 0.18\n8/8 [==============================] - 0s 3ms/step - loss: 6.0781 - accuracy: 0.1800\nEpoch #40\t Training Loss: 7.06\tTraining Accuracy: 0.17\tValidation Loss: 6.08\tValidation Accuracy: 0.18\n8/8 [==============================] - 0s 2ms/step - loss: 5.5827 - accuracy: 0.2400\nEpoch #41\t Training Loss: 6.75\tTraining Accuracy: 0.20\tValidation Loss: 5.58\tValidation Accuracy: 0.24\n2023-07-20 15:59:55.448572: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n2023-07-20 15:59:55.456349: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 992us/step - loss: 5.6449 - accuracy: 0.2360\nEpoch #42\t Training Loss: 6.81\tTraining Accuracy: 0.19\tValidation Loss: 5.64\tValidation Accuracy: 0.24\n8/8 [==============================] - 0s 973us/step - loss: 5.6111 - accuracy: 0.2320\nEpoch #43\t Training Loss: 6.96\tTraining Accuracy: 0.20\tValidation Loss: 5.61\tValidation Accuracy: 0.23\n8/8 [==============================] - 0s 975us/step - loss: 5.6648 - accuracy: 0.2320\nEpoch #44\t Training Loss: 6.94\tTraining Accuracy: 0.20\tValidation Loss: 5.66\tValidation Accuracy: 0.23\n8/8 [==============================] - 0s 1ms/step - loss: 5.6564 - accuracy: 0.2320\nEpoch #45\t Training Loss: 6.93\tTraining Accuracy: 0.20\tValidation Loss: 5.66\tValidation Accuracy: 0.23\n2023-07-20 15:59:56.093223: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n2023-07-20 15:59:56.283472: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 974us/step - loss: 5.6492 - accuracy: 0.2320\nEpoch #46\t Training Loss: 6.76\tTraining Accuracy: 0.20\tValidation Loss: 5.65\tValidation Accuracy: 0.23\n8/8 [==============================] - 0s 1ms/step - loss: 5.7075 - accuracy: 0.2320\nEpoch #47\t Training Loss: 6.76\tTraining Accuracy: 0.20\tValidation Loss: 5.71\tValidation Accuracy: 0.23\n8/8 [==============================] - 0s 1ms/step - loss: 5.7934 - accuracy: 0.2320\nEpoch #48\t Training Loss: 6.75\tTraining Accuracy: 0.20\tValidation Loss: 5.79\tValidation Accuracy: 0.23\n2023-07-20 15:59:56.606203: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n8/8 [==============================] - 0s 1ms/step - loss: 5.7843 - accuracy: 0.2320\nEpoch #49\t Training Loss: 6.74\tTraining Accuracy: 0.19\tValidation Loss: 5.78\tValidation Accuracy: 0.23\n8/8 [==============================] - 0s 980us/step - loss: 5.7775 - accuracy: 0.2320\nEpoch #50\t Training Loss: 6.74\tTraining Accuracy: 0.19\tValidation Loss: 5.78\tValidation Accuracy: 0.23\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"id":"4GipGyEkBQdj","colab":{},"colab_type":"code","source_hash":"29dc4eb9","execution_start":1689868801180,"execution_millis":70,"deepnote_to_be_reexecuted":false,"cell_id":"0c457df84bc44fea8374aac710cc8491","deepnote_cell_type":"code"},"source":"#Check Performance on Test Set\ntest_loss, test_accuracy = model.evaluate(X_data_test, y_labels_test)","execution_count":132,"outputs":[{"name":"stdout","text":"8/8 [==============================] - 0s 965us/step - loss: 5.5927 - accuracy: 0.2480\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"maTNvEQpbkoS","colab_type":"text","cell_id":"7f793347c5584e91bda1ccaed971015f","deepnote_cell_type":"markdown"},"source":"Now modify the existing model even more, and try to find the highest and appropriate testing and validation accuracy!"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7a947772-3ac4-4afb-803e-f093dcf9043f' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NeuralNetworks_Exercises.ipynb","provenance":[],"collapsed_sections":[]},"deepnote":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"deepnote_notebook_id":"a2197c8268d44d9793a09b80e384de41","deepnote_execution_queue":[]}}