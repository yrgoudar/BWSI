{"cells":[{"cell_type":"markdown","metadata":{"id":"9XZEvoDUdSKh","colab_type":"text","cell_id":"f6e79ff69a1a444c98e111296f8da041","deepnote_cell_type":"markdown"},"source":"# Machine Learning with Signals Problem Set\n\nPer usual, we are going to start by downoloading any packages or data that we need and importing all of the Python packages we will need, too."},{"cell_type":"code","metadata":{"id":"3MIrMPcldSQZ","colab":{"height":51,"base_uri":"https://localhost:8080/"},"outputId":"45bca704-e970-42e1-9dee-e7c7193ed38d","colab_type":"code","source_hash":null,"execution_start":1689791059888,"execution_millis":5128,"deepnote_to_be_reexecuted":false,"cell_id":"6d7ff77ff8b248b98885d59bf96cce00","deepnote_cell_type":"code"},"source":"# Install Graphvis library if it is not already installed\n!pip install graphviz \n\n# Clone the audio data repository (should cleanly fail if already downloaded)\n!git clone https://github.com/Jakobovski/free-spoken-digit-dataset.git\n\n# Define the path to where the data are downloaded\ndata_dir = '/work/Week2/04_SignalsML/free-spoken-digit-dataset/recordings/'","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: graphviz in /root/venv/lib/python3.9/site-packages (0.20.1)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mfatal: destination path 'free-spoken-digit-dataset' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"id":"dY2O4r5bW-j7","colab":{"height":71,"base_uri":"https://localhost:8080/"},"outputId":"0b48d12f-b988-4294-edf6-c06c83cc5e75","colab_type":"code","source_hash":null,"execution_start":1689791065021,"execution_millis":2349,"deepnote_to_be_reexecuted":false,"cell_id":"54e389d5f61c452cab14db1d490751c3","deepnote_cell_type":"code"},"source":"# Import basic system and numerical packages\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\n\n# Import statistical and signal processing functions\nfrom scipy import signal\nimport scipy.stats.mstats as mstats\nfrom sklearn import metrics\n\n# Import the scikit-learn functions and models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\n\n# Import audio i/o and playing functions\nimport scipy.io.wavfile\nfrom IPython.display import Audio\n\n# Import plotting functionality\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport graphviz\nimport seaborn as sns\n\n# Setup the plotting preferencs\n%matplotlib inline\nmatplotlib.rcParams.update({'font.size': 16})","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-DpBRUp2ubJC","colab_type":"text","cell_id":"e55d55a8aa884217a5a0d3ed980525cb","deepnote_cell_type":"markdown"},"source":"##Problem 1:\n$\\text{Ben Bitdiddle}$, an amateur data scientist, is working on a difficult classication problem given by his boss $\\text{Alyssa P. Hacker}$. Eventually, he comes up with a few features that be beleives are best of the best for his data and plots them below. Based on the visualization in the plot below, which feature is the most useful and which one is the least useful, why? Is Ben really the best?\n\n<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/Images/Week2/good_or_bad_features.png\">\n"},{"cell_type":"markdown","metadata":{"id":"2HG9jkWle0cv","colab_type":"text","cell_id":"1f480b10bb104f819758276b625a734b","deepnote_cell_type":"markdown"},"source":"X1 is the most useful because there is the least overlap between classes in the distribution plot. X3 is the least useful because it has the most overlap."},{"cell_type":"markdown","metadata":{"id":"TpWVDR_D0gYh","colab_type":"text","cell_id":"99e839b400824ec9983f35ccbb9c930f","deepnote_cell_type":"markdown"},"source":"##Problem 2\n\n  Here we are revisiting our dataset of recordings of spoken digtis (0-9). Here we will add some twists! Remember to visualize and really think about what features may or may not be useful!"},{"cell_type":"code","metadata":{"id":"qvpcS7sshZzF","colab":{"height":105,"base_uri":"https://localhost:8080/"},"outputId":"66bfa79a-dc52-4e71-8628-5e8d44f7ad77","colab_type":"code","source_hash":null,"execution_start":1689791067361,"execution_millis":19,"deepnote_to_be_reexecuted":false,"cell_id":"5e99a027c4bc4fc89b6bece61c72a7fc","deepnote_cell_type":"code"},"source":"# Let's double check that get got all of the files we expected\nfile_list = os.listdir(data_dir)\nprint('{} audio samples\\n'.format(len(file_list)))\n\n# list the first few files\nprint('Example files...')\nprint(file_list[0:10])","execution_count":null,"outputs":[{"name":"stdout","text":"3000 audio samples\n\nExample files...\n['8_nicolas_35.wav', '5_lucas_3.wav', '1_nicolas_23.wav', '2_jackson_32.wav', '6_george_4.wav', '2_nicolas_28.wav', '0_jackson_41.wav', '1_jackson_39.wav', '5_nicolas_14.wav', '7_george_38.wav']\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"id":"jPEORGagajex","colab_type":"text","cell_id":"231480be10c34e408d7bfc040f32d697","deepnote_cell_type":"markdown"},"source":"## Feature Engineering\nAs we saw earlier, some features are more useful for classification than others. Here, we provide you with functions to extract the \"spectral centroid\" and \"spectral flatness\" features from an audio file, but we will also help you start to build your own feature extracting functions."},{"cell_type":"markdown","metadata":{"id":"p4pwjmMHcjk5","colab_type":"text","cell_id":"f70cf5b873444277a915cb029e760a14","deepnote_cell_type":"markdown"},"source":"### Spectral Centroid\n\nThe [**spectral centroid**](https://en.wikipedia.org/wiki/Spectral_centroid) indicates at which frequency the energy of a spectrum is centered upon. This is like a weighted mean:\n$$f_c=\\frac{\\sum_kA(k)f(k)}{\\sum_kA(k)}$$\n \nwhere $A(k)$ is the spectral magnitude at frequency bin $k$,  $f(k)$ is the frequency at bin $k$\n ."},{"cell_type":"code","metadata":{"id":"ThhUeXOCYXAN","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689791067365,"execution_millis":6,"deepnote_to_be_reexecuted":false,"cell_id":"89665b47aa6a44f6bf0e70f9999b7853","deepnote_cell_type":"code"},"source":"def spectral_centroid(ft, f_s):\n    \"\"\"\n    Computes the spectral centroid from the FT of a signal\n\n    :param ft: output of Fourier Transform (i.e., np.fft.fft())\n    :param f_s: sampling frequency (or `sampling rate`) (in Hz)\n    \"\"\"\n    # Generate the frequencies associated with the fourier transform values\n    num_samples = len(ft)\n    freqs = np.linspace(0, f_s, num_samples)\n\n    # Grab the magnitude of the relevant part of the fourier transform values\n    freqs = freqs[0:num_samples//2]\n    magnitude = np.abs(ft[0:num_samples//2]) * (2/num_samples)\n    \n    # Compute the weighted frequency to get the specral centroid\n    spec_centroid = np.sum(magnitude*freqs)/np.sum(magnitude)\n\n    return spec_centroid","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"POEqXNK7lbUu","colab_type":"text","cell_id":"9466f7a45f7d4e549f440cb857ab7fcc","deepnote_cell_type":"markdown"},"source":"### Spectral Flatness\n\nThe [**spectral flatness**](https://en.wikipedia.org/wiki/Spectral_flatness), also known as Wiener entropy, is a measure used in digital signal processing to characterize an audio spectrum. Spectral flatness is typically measured in decibels, and provides a way to quantify how noise-like a sound is, as opposed to being tone-like."},{"cell_type":"code","metadata":{"id":"LrpDbTp8bZWR","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689791067410,"execution_millis":5,"deepnote_to_be_reexecuted":false,"cell_id":"cf2a93f4ea6e4a5b967215f45d003439","deepnote_cell_type":"code"},"source":"def spectral_flatness(ft):\n    \"\"\"\n    Computes the spectral flatness of the FT of a signal\n    \n    :param ft: output of Fourier Transform (i.e., np.fft.fft())\n    \"\"\"\n    # Grab the magnitude of the relevant part of the fourier transform values\n    num_samples = len(ft)\n    magnitude = abs(ft[0:num_samples//2]) * (2/num_samples)\n \n    # Spectral flatness is simply the geometric mean of the FT magnitude\n    # divided by the arithmetic mean of the FT magnitude\n    spec_flatness = mstats.gmean(magnitude)/np.mean(magnitude)\n\n    return spec_flatness","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blizQbTb8SU3","colab_type":"text","cell_id":"b75fb20ec49a448b82e4dd10b63b18b0","deepnote_cell_type":"markdown"},"source":"### Spectral Roll-off\nIn this section, you are going to write the code to extract a new feature called \"spectral roll-off\". This measure can used to measure the distinguish voice from non-voice audio. This can be used to give a good idea of what frequencies contain the majority of the energy. Spectral roll-off can be computed from the inequality:\n\n$$\\sum^{r}_{k=0} A(k) \\geq 0.85\\sum^{N}_{k=0} A(k) \\text{,}$$\n\nwhere $A(k)$ is the magnitude of the fourier transform at index $k$.\n\nThe idea is to find the frequency associated with index $r$, where the sum to the magnitudes in the fourier transform from $k=0$ up to $k=r$ is greater than or equal to $85$ percent of the sum the magnitudes from $k=0$ to $k=N$."},{"cell_type":"code","metadata":{"id":"RrrsRVURlr-m","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689791067411,"execution_millis":22,"deepnote_to_be_reexecuted":false,"cell_id":"aef972a7d6ed432a8cb5ae29cd56c8e1","deepnote_cell_type":"code"},"source":"def spectral_rolloff(ft, f_s):\n    # Generate the frequencies associated with the fourier transform values\n    num_samples = len(ft) # <-- UPDATE THESE LINES\n    freqs = np.linspace(0, f_s, num_samples)\n\n    # Grab the magnitude of the relevant part of the fourier transform values\n    freqs = freqs[:num_samples//2]       # <-- UPDATE THESE LINES\n    magnitude = np.abs(ft[:num_samples//2])\n\n    # Compute the sum of *all* the fourier transform magnitudes\n    whole_sum = np.sum(magnitude)   # <-- UPDATE THIS LINE\n\n    # Initialize a variable to hold the cumulative sum of the magnitudes of the\n    # fourier transform as we loop over it.\n    cumulative_sum = 0.0\n\n    # Loop over each element in the fourier transform\n    for k, ft_mag in enumerate(magnitude):\n        # Add the current magnitude of the cumulative sum\n        cumulative_sum += ft_mag     # <-- UPDATE THIS LINE\n\n        # Check if the cumulative sum is currently greater than 85% of the\n        # whole sum, and return the frequency at which this inequality first\n        # becomes true.\n        if cumulative_sum >= 0.85*whole_sum:\n            return freqs[k]","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGcKtBn8U99c","colab_type":"text","cell_id":"1380da3027f04bbcadbfe5b61008f393","deepnote_cell_type":"markdown"},"source":"#Implement your own feature extractors\nAdd your own feature! We've added some common techniques based on some popular audio signal processing methods. However, there are plenty out there. The links below contain some information regarding a few possible features to use (including some we have already implemented).\n\nSourc 1: https://musicinformationretrieval.com/spectral_features.html\n\nSource 2: https://www.cs.ccu.edu.tw/~wtchu/courses/2014f_MCA/Lectures/Lecture%209%20Audio%20and%20Music%20Analysis%202.pdf\n\nThere are lots of features to choose from, so pick a few that you think mightc help classify an audio clip (or feel free to create your own ;-) ). \n\nA few possible features from these sources:\n* Spectral Bandwidth\n* Spectral Flux\n* Zero Crossing Rate\n* Spectral Contrast\n  - This one is tricky because it requires using some form of the \"spectrogram\" (not just the fourier transform), but it also contains *much* more information and may be helpful for signal classification.\n\nOther ideas:\n* The \"Maximum Frequency Value\" shown in the tutorial stil exists. We are not using it here, but don't forget it exists.\n* Weighted standard deviation of the fourier transform\n   - This would be analogous to the spectral centroid but for standard deviation\n* Break up frequency into several \"frequency subbands\", compute centroid within each subband, and use each of those centroid values as a feature\n* Use the *number* of different peaks in the fourier transform as a feature\n* The sky is the limit, so get creative!\n"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689791067412,"execution_millis":8,"deepnote_to_be_reexecuted":false,"cell_id":"e3886951444d4ebfba4f409e58c47d03","deepnote_cell_type":"code"},"source":"def spectral_bandwidth_manual(s, f_s, p=2):\n    N = len(s)\n    f = np.linspace(0, f_s, N)\n    fc = spectral_centroid(s, f_s)\n\n    total = 0.0\n    for k in range(N):\n        total += s[k] * ((f[k] - fc) ** p)\n    \n    return total ** (1/p)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfgICUuZVXjy","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689791360420,"execution_millis":13,"deepnote_to_be_reexecuted":false,"cell_id":"e7d9b8c759b347e8971b3117accd0993","deepnote_cell_type":"code"},"source":"import librosa\ndef feature1_extractor(s, f_s):\n    S = []\n    for i in s:\n        S.append(float(i))\n    return np.mean(librosa.feature.spectral_bandwidth(y=np.array(S), sr=float(f_s), n_fft = len(S))[0])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689791067431,"execution_millis":20,"deepnote_to_be_reexecuted":false,"cell_id":"d1dca550a3c743c2b895921e2d1ff961","deepnote_cell_type":"code"},"source":"'''def spec_band(s, f_s):\n    S = []\n    for i in s:\n        S.append(float(i))\n    #return librosa.feature.spectral_bandwidth(y=np.array(S), sr=float(f_s), n_fft=)'''","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"'def spec_band(s, f_s):\\n    S = []\\n    for i in s:\\n        S.append(float(i))\\n    #return librosa.feature.spectral_bandwidth(y=np.array(S), sr=float(f_s), n_fft=)'"},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"UWD7GhXWw3Rw","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689791067438,"execution_millis":16,"deepnote_to_be_reexecuted":false,"cell_id":"6102dff5025a4c758ec1b65f33d3bb3b","deepnote_cell_type":"code"},"source":"def feature2_extractor(s, f_s):\n    \"\"\"The even cooler feature extractor I wrote\"\"\"\n    Z = 0\n    for i in range(1, len(s)):\n        Z += abs(np.sign(s[i])-np.sign(s[i-1]))\n    return 0.5*Z","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mPEZTcSkYnZ","colab_type":"text","cell_id":"e573d843d9f046ffb022b843c021816f","deepnote_cell_type":"markdown"},"source":"## Load full dataset and extract features"},{"cell_type":"code","metadata":{"id":"C2nB-pLjoVlU","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689791126101,"execution_millis":87841,"deepnote_to_be_reexecuted":false,"cell_id":"5a17467d0bec4e349705664b536286e1","deepnote_cell_type":"code"},"source":"# We can use the regular expression functionality (`re` package) to robustly\n# parse the name of each audio file to retrieve the digit spoken, the person\n# speaking, and the trial number of that digit-speaker pair.\n#\n# The following lines create little \"string parsers\" to grab that information.\nre_digit = re.compile('\\d+_')\nre_speaker = re.compile('_[a-z]+_')\nre_trial = re.compile('_\\d.')\n\n# Load this information into a dataframe and create columns to hold the features\n# Spectral Centroid (SC)\n# Spectral Flatness (SF)\n##########################################################\n# ADD THE NAMES OF YOUR FEATURES TO THIS LIST OF COLUMNS #\n##########################################################\ndf = pd.DataFrame(columns=['digit','speaker', 'SC','SF', 'SB', 'ZCR'])\n\n# Loop over each audio file in the data directory\nfor audio_file in os.listdir(data_dir):\n    # Try to load the file and parse all of its contents.\n    # If something goes wrong, Python will execute the contents of the \"except\"\n    # codeblock.\n    try:\n        # Use the same string parsers from before and grab info from filename\n        digit = int(re.match(re_digit, audio_file)[0][:-1])\n        speaker = re.search(re_speaker, audio_file)[0][1:-1]\n\n        # Read in the audio file\n        full_path_to_audio_file = os.path.join(data_dir, audio_file)\n        (sample_rate, y) = scipy.io.wavfile.read(full_path_to_audio_file)\n        \n        # If the audio recording is in stereo (2-channels), just use one of them\n        if len(y.shape) == 2:\n            y = y[:,0]\n\n        # Grab the number of samples, compute the sample interval, and generate\n        # the time-stamps for each of the audio samples\n        num_samples = len(y)\n        sample_interval = 1.0/sample_rate\n        t = np.arange(0, num_samples/sample_rate, sample_interval)\n\n        # Compute the Fourier Transform of the audio signal\n        ft = np.fft.fft(y)\n\n        # Calculate the audio features to be stored in the data frame\n        sc = spectral_centroid(ft, sample_rate)\n        sf = spectral_flatness(ft)\n        sb = feature1_extractor(y, sample_rate)\n        zcr = feature2_extractor(y, sample_rate)\n        ### OPTIONAL: ADD MORE FEATURES HERE!\n        ###\n        ### NOTE: IF YOUR FEATURE EXTRACTOR RETURNS MORE THAN ONE VALUE, THEN\n        ### YOU WILL NEED TO SEPARATE THOSE OUT INTO INDIVIDUAL SCALAR VARIABLES\n\n        # Add the info for this file to our dataframe\n        feature_dict = {'digit':digit, 'speaker':speaker,'SC':sc,'SF':sf,\n                        'SB':sb, # <-- UPDATE FEATURE NAME\n                        'ZCR':zcr} # <-- UPDATE FEATURE NAME\n                                             # OPTIONAL: EXTEND THE DICTIONARY\n                                             # WITH EVEN MORE FEATURES!\n        df = df.append(feature_dict, ignore_index=True)\n\n    except Exception as err:\n        # Something went wrong!  =(\n        # Notife the user of the audio file that broke and what the error was\n        print(audio_file)\n        raise err","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1-LovEVqnh2","colab":{"height":629,"base_uri":"https://localhost:8080/"},"outputId":"fd87afdf-cea8-4c73-aa34-c5b5dd548087","colab_type":"code","source_hash":null,"execution_start":1689791238471,"execution_millis":112,"deepnote_to_be_reexecuted":false,"cell_id":"9608069ac6704e7f997d1508cd4eeb6d","deepnote_cell_type":"code"},"source":"# Print the head of the dataframe\nprint('df.head():\\n')\nprint(df.head(15))\n\n# Also print some summary information about the dataframe\nprint('\\n\\ndf.info():\\n')\nprint(df.info())","execution_count":null,"outputs":[{"name":"stdout","text":"df.head():\n\n   digit   speaker           SC        SF  \\\n0      8   nicolas  1160.920727  0.480644   \n1      5     lucas   951.610951  0.293918   \n2      1   nicolas  1179.713806  0.528961   \n3      2   jackson   794.259527  0.253679   \n4      6    george  1939.407395  0.348809   \n5      2   nicolas  1011.517350  0.398295   \n6      0   jackson   802.618018  0.287560   \n7      1   jackson  1096.281780  0.366212   \n8      5   nicolas  1082.734343  0.502244   \n9      7    george  1424.554517  0.406677   \n10     7      theo  1275.965936  0.460095   \n11     4      theo   732.173447  0.256113   \n12     9  yweweler   969.760340  0.341572   \n13     7  yweweler  1290.708710  0.556307   \n14     8    george  1949.068986  0.445941   \n\n                                                   SB     ZCR  \n0   [1005.2042014503768, 997.845812322174, 990.288...   453.0  \n1   [673.1580690993023, 649.2553059135399, 651.293...  1094.0  \n2   [1137.0018355535851, 1070.6516533036106, 1105....   456.0  \n3   [1215.3599435293595, 1154.4935138098194, 998.5...   803.0  \n4   [1176.495822602282, 1166.7805223444323, 1158.6...  1879.0  \n5   [1373.0767679728517, 1228.465361178981, 1109.6...   477.0  \n6   [1080.4738449585307, 1086.415678583715, 1038.6...   438.0  \n7   [1046.5740144416818, 1108.6364668940278, 1126....   393.0  \n8   [904.4930687050572, 920.0255215991455, 952.758...   628.0  \n9   [1015.0571221709125, 994.2459843114947, 988.00...  1137.0  \n10  [942.1372356732467, 925.9479217525393, 918.850...   453.0  \n11  [596.3743375656378, 618.5646945613705, 607.283...   233.0  \n12  [729.8534643878503, 687.2418820201367, 674.640...   526.0  \n13  [1035.3546199395853, 1025.2980745784914, 1009....   404.0  \n14  [1114.5662408684511, 1120.7826070146348, 1177....   644.0  \n\n\ndf.info():\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3000 entries, 0 to 2999\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   digit    3000 non-null   object \n 1   speaker  3000 non-null   object \n 2   SC       3000 non-null   float64\n 3   SF       3000 non-null   float64\n 4   SB       3000 non-null   object \n 5   ZCR      3000 non-null   float64\ndtypes: float64(3), object(3)\nmemory usage: 140.8+ KB\nNone\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689778706056,"execution_millis":36,"deepnote_to_be_reexecuted":true,"cell_id":"43c4a319156c4b6b89c21e04af4d96cd","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689791250352,"execution_millis":7,"deepnote_to_be_reexecuted":false,"cell_id":"45c568112a5a46dd94318a6bbc422460","deepnote_cell_type":"code"},"source":"df['SB']","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"0       [1005.2042014503768, 997.845812322174, 990.288...\n1       [673.1580690993023, 649.2553059135399, 651.293...\n2       [1137.0018355535851, 1070.6516533036106, 1105....\n3       [1215.3599435293595, 1154.4935138098194, 998.5...\n4       [1176.495822602282, 1166.7805223444323, 1158.6...\n                              ...                        \n2995    [1116.8594926740732, 1122.1291531382694, 1131....\n2996    [1376.4465611714995, 1303.2980519903265, 1155....\n2997    [986.4807288827894, 928.9289689294865, 818.828...\n2998    [778.1733253558455, 737.8005312386919, 751.030...\n2999    [968.2708698600595, 958.798358596892, 951.6886...\nName: SB, Length: 3000, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"OSKTSban0xt7","colab_type":"text","cell_id":"5cef8da75af4446a8bed47de694ba5ec","deepnote_cell_type":"markdown"},"source":"## Considering Your Features\n\nNow that we have successfully extracted a bunch of features from the audio files in this database, let's pause and examine the results."},{"cell_type":"code","metadata":{"id":"r_k_wABdu2YI","colab":{"height":1000,"base_uri":"https://localhost:8080/"},"outputId":"8e41c71f-9729-41ca-a701-74b7fd7ba9ec","colab_type":"code","source_hash":null,"execution_start":1689779475747,"execution_millis":54461,"deepnote_to_be_reexecuted":true,"cell_id":"22afe0ab0996437798ac32f77b8947bc","deepnote_cell_type":"code"},"source":"# Visualize a pairplot of the spectral features. In this instance, color each\n# datapoint by the *person* speaking the digit to see if each *speaker* has a\n# distinguishable set of spectral characteristics (e.g., deep vs high voice)\ng = sns.pairplot(data=df, vars=['SC', 'SF', 'SB', 'ZCR'],\n                 hue='speaker', corner=True)\nplt.suptitle('Spectral Features (colored by speaker)', x=0.5, y=1.02)\n\n# Clean up the plot appearance\nplt.gcf().tight_layout()\n\n# Repeat this plot, but now color each datapoint by the *digit* being spoken to\n# see if each *word* has a distinguishable set of spectral characteristics\n# (e.g., does the buzzing 'z' in 'zero' cause a different set of characteristics\n# than the shap 't' sound in 'two'?)\ng = sns.pairplot(data=df, vars=['SC', 'SF', 'SB', 'ZCR'],\n                 hue='digit', corner=True)\nplt.suptitle('Spectral Features (colored by digit)', x=0.5, y=1.02)\n\n# Clean up the plot appearance\nplt.gcf().tight_layout()","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WrxYD_bs3U-k","colab_type":"text","cell_id":"da63a556d505436eb706fe426ec88f19","deepnote_cell_type":"markdown"},"source":"## Problem 3\n\nDoes it look like you have engineered some features that help separate the audio files by which digit was being spoken? What about by which person was speaking?"},{"cell_type":"markdown","metadata":{"id":"VHXrHOZS768Y","colab_type":"text","cell_id":"3ba39870816343b29d69504fa4351d01","deepnote_cell_type":"markdown"},"source":"SC, SF, and SB are the best for separating audio files by digit and person."},{"cell_type":"markdown","metadata":{"id":"Fh-Q8Cd6755u","colab_type":"text","cell_id":"b7f8eb0d84924fd1bd635645f49e6724","deepnote_cell_type":"markdown"},"source":"You can re-jigger your feature extractors however you want (or add more), and re-run the above cells to examine those effects on the feature space."},{"cell_type":"markdown","metadata":{"id":"nT74fm4M1eV-","colab_type":"text","cell_id":"f0a1a734ac40462f945ee84e401457bf","deepnote_cell_type":"markdown"},"source":"# Train Machine Learning Algorithm\n\nThe following section perfectly parallels the content from the tutorial. We are simply copying it over here to the exercises so that you can see if your features helped classify the audio data.\n\nYou are not *quite* done yet though. There are some thoughts in the \"Conclusion\" section you may want to consider."},{"cell_type":"code","metadata":{"id":"UdTb80ywsnIJ","colab":{"height":51,"base_uri":"https://localhost:8080/"},"outputId":"35064947-e442-4bea-828d-18f76b423943","colab_type":"code","source_hash":null,"execution_start":1689780939213,"execution_millis":12,"deepnote_to_be_reexecuted":true,"cell_id":"0b0439bfa7b74500bf025e8c0a31025a","deepnote_cell_type":"code"},"source":"# Select the name of the column to be used as the desired 'label' output\n# (For these files, it makes sense to use either 'speaker' or 'digit')\nlabel = 'speaker'\n\n# Create a list of features to be used in the classification process.\n# We use the 'set' object in Python to quickly remove the unwanted columns\n#\n# NOTE: Add any columns you *do not* want to be included in the training\n# algorithm to the \"columns_to_remove\" variable.\ncolumns_to_remove = set([label, 'file', 'trial'])\nfeatures = set(df.columns) - columns_to_remove\nfeatures = list(features)\n\nprint('Classifying {} using...'.format(label))\nprint('features: {}'.format(features))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GnBZfexpwggY","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689780939805,"execution_millis":25,"deepnote_to_be_reexecuted":true,"cell_id":"e63197d20a5f49749b09bda9152bf70d","deepnote_cell_type":"code"},"source":"# Convert the data in the 'speaker', 'digit', and 'trial' columns of the\n# dataframe into the 'Categorical' type.\ndf.speaker = pd.Categorical(df.speaker)\ndf.digit = pd.Categorical(df.digit)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwFYJAyAwp_B","colab":{"height":238,"base_uri":"https://localhost:8080/"},"outputId":"70432729-2709-49af-e584-b3e0556ba716","colab_type":"code","source_hash":null,"execution_start":1689780940359,"execution_millis":85,"deepnote_to_be_reexecuted":true,"cell_id":"2853546193c44f8e96652f36403b38d0","deepnote_cell_type":"code"},"source":"# If the speaker is in the set of features used to classify the audio files,\n# then the string speaker value should be converted to its numerical version.\nif 'speaker' in features:\n    # Add a column containing the numerically encoded version of speaker name\n    df['speaker_code'] = df.speaker.cat.codes\n\n    # Replace the 'speaker' feature with the 'speaker_code' feature\n    features.remove('speaker')\n    features.append('speaker_code')\n\n    # Double check that there are not duplicate entries in the list of features\n    # by converting to a `set` object (which automatically removes duplicate\n    # entries) then convert back `list` object.\n    features = list(set(features))\n\n# Update the user on what columns and content remain in the datafram\ndf.info()","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V4CH6ZfdzEbA","colab_type":"text","cell_id":"8a7fce49b7114aa1ba6d4233f8a5927a","deepnote_cell_type":"markdown"},"source":"Next we split the data into training and test sets."},{"cell_type":"code","metadata":{"id":"POmoLVIe1d2k","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689780942299,"execution_millis":34,"deepnote_to_be_reexecuted":true,"cell_id":"b63a2a3b5d094b1c9c45bcad205ada34","deepnote_cell_type":"code"},"source":"# split X and y into training and testing sets\n# NOTE: Using convention that X = features, y = label\nX_train, X_test, y_train, y_test = train_test_split(\n    df.loc[:,features], df[label], test_size=0.25,\n    random_state=0, stratify = df[label])","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PYx9GHfizKaS","colab_type":"text","cell_id":"b682e904dddc486eb905aaede2082cf0","deepnote_cell_type":"markdown"},"source":"Now, with the correct training features and labels, we can train a machine learning classification model. To start with, let's consider a decision tree."},{"cell_type":"code","metadata":{"id":"wvVI50ci1eAH","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689780943361,"execution_millis":613,"deepnote_to_be_reexecuted":true,"cell_id":"56097cffb2c34bce9d4a2693956341d0","deepnote_cell_type":"code"},"source":"# Instantiate a DecisionTree model\nmodel = tree.DecisionTreeClassifier(max_depth=4)\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l5TWpNhE1d9x","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689780943983,"execution_millis":933,"deepnote_to_be_reexecuted":true,"cell_id":"8e2bfe8a56e242d1a2dcfe9ad73197b2","deepnote_cell_type":"code"},"source":"# Convert each class name into a string representation\nclass_names = [str(c) for c in model.classes_]\n\n# Generate the data to visualize the decision tree\ndot_data = tree.export_graphviz(model, out_file=None, \n                         feature_names=features,  \n                         class_names=class_names,  \n                         filled=True, rounded=True) \n\n# Visualize the tree from that exported data\ngraph = graphviz.Source(dot_data)  \ngraph","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vy9xs8UU4XTq","colab_type":"text","cell_id":"35cb9ac7c5f940468e6da92d6d43a750","deepnote_cell_type":"markdown"},"source":"## Evaluate on Test Data"},{"cell_type":"code","metadata":{"id":"P7IgU64x6QAY","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689780945764,"execution_millis":54,"deepnote_to_be_reexecuted":true,"cell_id":"862f8d1240a74621aef0eb458f88d443","deepnote_cell_type":"code"},"source":"# Visualize the confusion matrix\ndef plot_cmatrix(cm,labels,title='Confusion Matrix'):\n    \"\"\"\n    Plot the confusion matrix for the classifier\n\n    :param cm: the actual confusion matrix\n    :param labels: the labels to add along the axes of the matrix\n    :param title: the title to place over the confusion matrix plot\n    \"\"\"\n    # Generate a new figure and axes instance\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # Display the confusion matrix (attempt to account for normalization)\n    row_totals = np.sum(cm, axis=1)\n    vmax_val = row_totals.max() if row_totals.max() > 1 else 1.0\n    ax_im = ax.imshow(cm, cmap='Reds', vmin=0, vmax=vmax_val)\n\n    # Annotate the figure\n    plt.title(title, fontsize=20)\n    fig.colorbar(ax_im)\n    ax.set_xticks(range(len(labels)))\n    ax.set_yticks(range(len(labels)))\n    ax.set_xticklabels(labels, fontsize=16, rotation=70)\n    ax.set_yticklabels(labels, fontsize=16)\n    plt.xlabel('Predicted', fontsize=16)\n    plt.ylabel('True', fontsize=16)\n    plt.show()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6nWEv1M4Xf9","colab":{},"colab_type":"code","source_hash":null,"execution_start":1689780946594,"execution_millis":1350,"deepnote_to_be_reexecuted":true,"cell_id":"7ea344b640d6400aa26d79e49b5dff6a","deepnote_cell_type":"code"},"source":"# Predict class label probabilities\nlabels=np.sort(y_test.unique())\ny_test_pred = model.predict(X_test)\n\ncm = metrics.confusion_matrix(y_test,y_test_pred, labels=labels)\nplot_cmatrix(cm, labels)\ncm","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_-B4Ay05qC2","colab_type":"text","cell_id":"29690c846b8b4cf998c0e2d82594cca1","deepnote_cell_type":"markdown"},"source":"# Conclusion\n\nFor consistency with the tutorial, we have used a decision tree to classify the data, but you could train a *different* classifier. In particular, do you recall if there is a classifier that may help separate features that are all mingled together like some of these features are?\n\nSVM"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7a947772-3ac4-4afb-803e-f093dcf9043f' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SignalsML_Exercises.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[]},"vscode":{"interpreter":{"hash":"db3da3f361baf1113dc5160ff8c57cf22bdd1b4d834d98df35e37ddb9b3b9b90"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3.7.7 ('ML')"},"language_info":{"name":"python","version":"3.7.7"},"deepnote_notebook_id":"9a3f9f8a7bbd40c290df2edb2a47fe54","deepnote_persisted_session":{"createdAt":"2023-07-19T18:46:22.525Z"},"deepnote_execution_queue":[]}}