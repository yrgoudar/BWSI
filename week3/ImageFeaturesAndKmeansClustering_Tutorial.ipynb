{"cells":[{"cell_type":"markdown","metadata":{"id":"coOy1a-DllSe","colab_type":"text","cell_id":"aadc6bdac63c434489e86d8cc7ecb3c7","deepnote_cell_type":"markdown"},"source":"# Tissue Classification using Texture Features\nIn this notebook, we will begin our exploration of image classification. For this example problem, we will be using an example set of [histological sample images](https://en.wikipedia.org/wiki/Histology) taken as part of a study on colorectal cancer. The full dataset and a description of its properties can be accessed at this site: http://dx.doi.org/10.5281/zenodo.53169\n\n![Example histological images for classes a through f](https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/Images/Week3/Representative-images-the-first-10-images-of-every-tissue-class.png?token=ADGRCSE2YIYWUNCJTHNB5XK67T4HE)\n\nIn the above figure, we can see examples of eight different classes present in the dataset. Each row, labeled \"a\" through \"h\", corresponds to a unique class of tissue. Each of the images in a given row represents a single example image of that class of tisue. The objective of the code in this notebook is to use the [k-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering) to group images from the dataset into these respective classes.","block_group":"aadc6bdac63c434489e86d8cc7ecb3c7"},{"cell_type":"markdown","metadata":{"id":"XyqVGg0bMtIR","colab_type":"text","cell_id":"470260f5f36f4ad9b7bf9383de372551","deepnote_cell_type":"markdown"},"source":"# Imports\nAs per usual, we will start by importing the packages and modules we will be needing for this project. In this case, we will import the usual Numpy and Matplotlib modules. We will also import some statistics modules from Scipy, and a number of utilities from the [scikit-learn](https://scikit-learn.org/stable/) and [scikit-image](https://scikit-image.org/) packages.","block_group":"470260f5f36f4ad9b7bf9383de372551"},{"cell_type":"code","metadata":{"tags":[],"source_hash":null,"execution_start":1690222514392,"execution_millis":5287,"deepnote_to_be_reexecuted":false,"cell_id":"bd178658bf6a4d12b7221c6f363c28a2","deepnote_cell_type":"code"},"source":"!pip install scikit-image","block_group":"bd178658bf6a4d12b7221c6f363c28a2","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting scikit-image\n  Downloading scikit_image-0.21.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting networkx>=2.8\n  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting imageio>=2.27\n  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 KB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tifffile>=2022.8.12\n  Downloading tifffile-2023.7.18-py3-none-any.whl (221 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 KB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy>=1.8 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-image) (1.9.3)\nCollecting PyWavelets>=1.1.1\n  Downloading PyWavelets-1.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting lazy_loader>=0.2\n  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\nRequirement already satisfied: pillow>=9.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-image) (9.2.0)\nRequirement already satisfied: numpy>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-image) (1.23.4)\nRequirement already satisfied: packaging>=21 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging>=21->scikit-image) (3.0.9)\nInstalling collected packages: tifffile, PyWavelets, networkx, lazy_loader, imageio, scikit-image\nSuccessfully installed PyWavelets-1.4.1 imageio-2.31.1 lazy_loader-0.3 networkx-3.1 scikit-image-0.21.0 tifffile-2023.7.18\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/9970e00d-58fe-400b-9b5f-5a62939fc73a","content_dependencies":null},{"cell_type":"code","metadata":{"id":"sMbWc0YFllSl","colab":{},"colab_type":"code","source_hash":null,"execution_start":1690222519588,"execution_millis":2418,"deepnote_to_be_reexecuted":false,"cell_id":"a1d4a81877e04ef1b5a70cff1af70852","deepnote_cell_type":"code"},"source":"# Basic operating system (os), numerical, and plotting functionality\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# scipy statistics functions\nfrom scipy.stats import mode\nfrom scipy.stats import moment\n\n# scikit-learn data utilities\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\n# Color transformations\nfrom skimage.color import rgb2lab\n\n# Feature extractors and classification algorithm\nfrom skimage.feature import greycomatrix, greycoprops\nfrom skimage.feature import local_binary_pattern\nfrom sklearn.cluster import KMeans\n\n# scikit-learn performance metric utilities\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","block_group":"a1d4a81877e04ef1b5a70cff1af70852","execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'greycomatrix' from 'skimage.feature' (/root/venv/lib/python3.9/site-packages/skimage/feature/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn [2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rgb2lab\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Feature extractors and classification algorithm\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m greycomatrix, greycoprops\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m local_binary_pattern\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'greycomatrix' from 'skimage.feature' (/root/venv/lib/python3.9/site-packages/skimage/feature/__init__.py)"]}],"outputs_reference":"s3:deepnote-cell-outputs-production/db5d2cbd-76d5-4277-978f-597450e720e8","content_dependencies":null},{"cell_type":"code","metadata":{"id":"qK45ObLW6YJF","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669401450,"execution_millis":1,"deepnote_to_be_reexecuted":true,"cell_id":"a0b980c02f91456e8456bd510460d5aa","deepnote_cell_type":"code"},"source":"# Set plotting preferences\nimport matplotlib\n%matplotlib inline\nfont = {'family' : 'sans-serif',\n        'weight' : 'normal',\n        'size'   : 16}\nmatplotlib.rc('font', **font)","block_group":"a0b980c02f91456e8456bd510460d5aa","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"Ta7q5xwIllSi","colab_type":"text","cell_id":"0445f5911f5945a1928943362c2ad06b","deepnote_cell_type":"markdown"},"source":"# Retrieve and Load the Data\nThe following cell checks if the histological images have already been downloaded into the current Colab session and downloads them if not.","block_group":"0445f5911f5945a1928943362c2ad06b"},{"cell_type":"code","metadata":{"id":"KJvH96N6J4tP","colab":{"height":102,"base_uri":"https://localhost:8080/"},"outputId":"2d8b07b0-6df2-4a9d-efea-9bdb158d5e99","colab_type":"code","source_hash":null,"execution_start":1626669401454,"execution_millis":5,"deepnote_to_be_reexecuted":true,"cell_id":"1b24af9d8a784d918d9826e0d7455d27","deepnote_cell_type":"code"},"source":"# Define the current directory and the directory where the files to download can\n# be found\ncurrent_dir = os.getcwd()\nremote_path = 'https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/'\n\n# Define and build a directory to save this data in\ndata_dir = os.path.join(current_dir, 'crc_data')\nif not os.path.isdir(data_dir):\n  os.mkdir(data_dir)\n\n# Move into the data directory and download all of the files\nos.chdir(data_dir)\nfor ii in range(1, 6):\n    basename = f'rgb0{ii}.npz'\n    filename = os.path.join(remote_path, basename)\n\n    # Check if the file has already been downloaded\n    if not os.path.isfile(basename):\n      cmd = f'wget {filename}'\n      print(cmd)\n      os.system(cmd)\n\n# Return to the original directory\nos.chdir(current_dir)","block_group":"1b24af9d8a784d918d9826e0d7455d27","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"tDGLNYFqJ90l","colab_type":"text","cell_id":"9ec22244c9d8422aa4f66393674f30cd","deepnote_cell_type":"markdown"},"source":"The downloaded \".npz\" archives can now be read into memory and parsed. The \".npz\" arhcives contain three entries.\n\n* The \"rgb_data\" entry contains the image data stored as a (1000, 150, 150, 3)\n    * These axes correspond to (image number, y-pixel , x-pixel, RGB color channel)\n* The \"labels\" entry contains the numerical representation of the class associated with each of the images (value 0-7)\n* \"labels_str\" entry contains a single dictionary which maps the numerical \"label\" value into a string representation. This entry is only present in the \"rgb01.npz\" archive.\n    * 0: '02_STROMA'\n    * 1: '06_MUCOSA'\n    * 2: '05_DEBRIS'\n    * 3: '01_TUMOR'\n    * 4: '03_COMPLEX'\n    * 5: '08_EMPTY'\n    * 6: '04_LYMPHO'\n    * 7: '07_ADIPOSE'\n\nThe cell below loops through each of the five archive files and concatenates the data in them. Additionally, only the red color channel is retained so that the final variables have the shapes\n\n\"images\" shape = (5000, 150, 150)\n\n\"label_str\" shape = (5000,)","block_group":"9ec22244c9d8422aa4f66393674f30cd"},{"cell_type":"markdown","metadata":{"id":"m93n6BbUxfQw","colab_type":"text","cell_id":"7cb9c575fb5b4d74a7fc27aae60771dc","deepnote_cell_type":"markdown"},"source":"Next, we define a function to use for loading the downloaded data into memory. This function also contains some lines to convert from the red-green-blue (r,g,b,) colorspace (which is the standard way to represent digital images in computers) to other colorspaces. The first colorspace is 'grayscale', which we compute by simply taking the arithmetic *average* of the (r, g, b) values. The other alternative colorspace is [CIE L\\*a\\*b\\*](https://en.wikipedia.org/wiki/CIELAB_color_space). This can be done with the \"rgb2lab\" function we imported at the top of this notebook.","block_group":"7cb9c575fb5b4d74a7fc27aae60771dc"},{"cell_type":"code","metadata":{"id":"Y9rC7eJMllSv","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669401467,"execution_millis":2,"deepnote_to_be_reexecuted":true,"cell_id":"75d91eea50d94687aa837b2978329f27","deepnote_cell_type":"code"},"source":"# Define a function to load the data from the assumed download path\ndef load_images(colorspace='rgb'):\n    \"\"\"\n    Loads the example data and applies transformation into requested colorspace\n\n    Arguments\n    ---------\n    colorspace : str, optional, default: `rgb`\n        The colorspace into which the images should be transformed. Accepted\n        values include\n\n        'rgb' : Standard red-green-blue color-space for digital images\n\n        'gray' or 'grey': An arithmetic average of the (r, g, b) values\n\n        'lab': The CIE L*a*b* colorspace\n    \n    Returns\n    -------\n    images : numpy.ndarray, shape (Nimg, Ny, Nx, Ncolor)\n        The complete set of transformed images\n\n    labels : numpy.ndarray, shape (Nimg)\n        The classification labels associated with each entry in `images`\n\n    label_to_str : dict\n        A dictionary which converts the numerical classification value in\n        `labels` into its string equivalent representation.\n    \"\"\"\n    # Check that the colorspace argument is recognized\n    colorspace_lower = colorspace.lower()\n    if colorspace_lower not in ['rgb', 'gray', 'grey', 'lab']:\n        raise ValueError(f'`colorspace` value of {colorspace} not recognized')\n\n    # Load data, which is stored as a numpy archive file (.npz)\n    filename = os.path.join(data_dir, 'rgb01.npz')\n    print(f'loading {filename}')\n    tmp = np.load(os.path.join(data_dir, 'rgb01.npz'), allow_pickle=True)\n\n    # Parse the loaded data into images and labels\n    # Initialize the images and labels variables using the first archive data\n    images = tmp['rgb_data']\n    if colorspace_lower == 'rgb':\n        pass\n    elif colorspace_lower in ['gray', 'grey']:\n        images = np.mean(images, axis=-1)      # Average into grayscale\n    elif colorspace_lower == 'lab':\n        images = rgb2lab(images)               # Convert to CIE L*a*b*\n\n    # Grab the initial array for the image labels\n    labels = tmp['labels']\n    \n    # Grab the dictionary to convert numerical labels to their string equivalent\n    label_to_str = tmp['label_str']\n    label_to_str = label_to_str.tolist() # Convert label_to_str into a dict\n\n    # Update the user on the number and size of images loaded\n    print('Loaded images with shape {}'.format(images.shape))\n    del tmp\n\n    # Loop over each of the remaining archives and append the contained data\n    for ii in range(2,6):\n        # Build the full path to the archive and load it into memory\n        filename = os.path.join(data_dir, f'rgb0{ii}.npz')\n        print(f'loading {filename}')\n        tmp = np.load(filename, allow_pickle=True)\n\n        # Parse and append the data\n        these_images = tmp['rgb_data']\n        if colorspace_lower == 'rgb':\n            pass\n        elif (colorspace_lower == 'gray') or (colorspace_lower == 'grey'):\n            these_images = np.mean(these_images, axis=-1) # Convert to grayscale\n        elif colorspace_lower == 'lab':\n            these_images = rgb2lab(these_images)          # Convert to CIEL*a*b*\n\n        # Append the images and labels\n        images = np.append(images, these_images, axis=0)\n        labels = np.append(labels, tmp['labels'], axis=0)\n\n        # Update the user on the number and size of images\n        print('Loaded images with shape {}'.format(these_images.shape))\n        del tmp\n\n    # Force the image data to be floating point and print the data shape\n    images = images.astype(np.float)\n    print('Final image data shape: {}'.format(images.shape))\n    print('Number of image labels: {}'.format(*labels.shape))\n\n    return images, labels, label_to_str","block_group":"75d91eea50d94687aa837b2978329f27","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"1LGPaPqqrSoQ","colab_type":"text","cell_id":"15def023dc4f42538e8f495e7e5ef7c2","deepnote_cell_type":"markdown"},"source":"With this data-loading function defined, let's load in the grayscale images.","block_group":"15def023dc4f42538e8f495e7e5ef7c2"},{"cell_type":"code","metadata":{"id":"_v912cBGrR4D","colab":{"height":221,"base_uri":"https://localhost:8080/"},"outputId":"54393cca-99f4-4c25-db5f-dc9544dacc63","colab_type":"code","source_hash":null,"execution_start":1626669401471,"execution_millis":4886,"deepnote_to_be_reexecuted":true,"cell_id":"73db064a8b1c4395b613be7e73e4ef76","deepnote_cell_type":"code"},"source":"# Load the data using our data-loading function\nimages_gray, labels, label_to_str = load_images(colorspace='gray')","block_group":"73db064a8b1c4395b613be7e73e4ef76","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"eGGzS66VToeY","colab_type":"text","cell_id":"f4c58c8e3249499287999240c4bb33be","deepnote_cell_type":"markdown"},"source":"As a sanity check, we pause here to examine the first image in the dataset.","block_group":"f4c58c8e3249499287999240c4bb33be"},{"cell_type":"code","metadata":{"id":"rYNV9u2GllS2","colab":{"height":268,"base_uri":"https://localhost:8080/"},"outputId":"38c41402-17c5-4df4-c316-98e43bd34d64","colab_type":"code","source_hash":null,"execution_start":1626669406362,"execution_millis":114,"deepnote_to_be_reexecuted":true,"cell_id":"72da87af97e74e62ab65d4321410cc77","deepnote_cell_type":"code"},"source":"# Show a sample image with its classification\nex_img_num = 1\nplt.imshow(images_gray[ex_img_num], cmap='gray')\nplt.title(label_to_str[labels[ex_img_num]])\nplt.axis('off')\nplt.show()","block_group":"72da87af97e74e62ab65d4321410cc77","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"O45zIL8lllS-","colab_type":"text","cell_id":"4f4cfe0248f04fa192d17977af516ec5","deepnote_cell_type":"markdown"},"source":"# Generating Image Features\nNext, we experiment with some possible features to  be used for classification.","block_group":"4f4cfe0248f04fa192d17977af516ec5"},{"cell_type":"markdown","metadata":{"id":"XbBszu4x2Blz","colab_type":"text","cell_id":"65efb7703a984187a5e723aad01091ee","deepnote_cell_type":"markdown"},"source":"## Pixel grayscale values\nAs a first approach, we are going to try using the raw, grayscale image pixel values as input features in a k-means clustering algorithm and see how well it performs. To do this, we will need to flatten each of the 150x150 pixel images into a vector of length 22500. Thus, the information in each image will be represented by a single vector in a 22500 dimensional \"image space.\"","block_group":"65efb7703a984187a5e723aad01091ee"},{"cell_type":"code","metadata":{"id":"xFxqpDYMllS_","colab":{"height":34,"base_uri":"https://localhost:8080/"},"outputId":"4fc85ac3-f419-47ba-d4a8-27fa1e987956","colab_type":"code","source_hash":null,"execution_start":1626669406470,"execution_millis":9,"deepnote_to_be_reexecuted":true,"cell_id":"0464ba5a391b48718e091ee5e7b7e6b8","deepnote_cell_type":"code"},"source":"# Flatten the images into a list of vectors\nnum_images, nrows, ncols = images_gray.shape\nimages_gray = images_gray.reshape((num_images, nrows*ncols))\nprint('{} images flattened into vectors of length {}'.format(*images_gray.shape))","block_group":"0464ba5a391b48718e091ee5e7b7e6b8","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"2oU8qeyjC3JX","colab_type":"text","cell_id":"ae06ffe702684f9bb6f1c1b663f30505","deepnote_cell_type":"markdown"},"source":"We can use the scikit-learn \"train_test_split\" function to randomly divide the images and labels into training and testing sets. The fraction of the data reserved as a test set is determined by the \"\ntest_size\" keyword argument of the \"train_test_split\" function. For this example, we are reserving 25% of the images and associated labels as a test set.","block_group":"ae06ffe702684f9bb6f1c1b663f30505"},{"cell_type":"code","metadata":{"id":"O3TuiJSjvBtX","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669406473,"execution_millis":252,"deepnote_to_be_reexecuted":true,"cell_id":"0a99b6c089294a839cecd81b49661628","deepnote_cell_type":"code"},"source":"# Split the data into training and testing data\n# NOTE: using convention of \"X\" as data and \"y\" as label\nX_train_gray, X_test_gray, y_train, y_test = train_test_split(images_gray, labels, test_size=0.25)\n\n# Start by clearing out the old, grayscale representation of the data\ndel images_gray","block_group":"0a99b6c089294a839cecd81b49661628","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"fsbYa89ndxnN","colab_type":"text","cell_id":"56eb9a62983a4d5187dd5a3dbc81b17f","deepnote_cell_type":"markdown"},"source":"Next, we build a KMeans clustering estimator with eight clusters (corresponding to the eight image classes) and pass the training image vectors through the \"fit()\" method. This method loops through the k-means iterations group the images non-overlaping clusters in the 22500 dimensional image space. This training method attempts to divide the input images into eight non-overlaping clusters. However, it is important to note that the training labels were not fed into the \"fit()\" method, so the result of the kmeans algorithm does not immediately inform the user what . Instead, it only identifies eight groupings of images and assigns them an effective \"cluster number.\"","block_group":"56eb9a62983a4d5187dd5a3dbc81b17f"},{"cell_type":"code","metadata":{"id":"TgUNIZfd3fSA","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669406728,"execution_millis":27239,"deepnote_to_be_reexecuted":true,"cell_id":"d0f2f9293ed74bd7800af1bd12bfd23a","deepnote_cell_type":"code"},"source":"# Run the KMeans clustering algorithm\nkmeans_estimator = KMeans(n_clusters=8, max_iter=10).fit(X_train_gray)","block_group":"d0f2f9293ed74bd7800af1bd12bfd23a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"9H3yP3_oSmG3","colab_type":"text","cell_id":"fb9bbc4c1b544ff9a15db93cd7bcfe34","deepnote_cell_type":"markdown"},"source":"Next, predict a cluster number for each of the test images based on the clustering boundaries estimated from the training images.","block_group":"fb9bbc4c1b544ff9a15db93cd7bcfe34"},{"cell_type":"code","metadata":{"id":"FPYGPSoUShHn","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669434010,"execution_millis":171,"deepnote_to_be_reexecuted":true,"cell_id":"f5637cb4d32d4ab38afa6029ad5482ba","deepnote_cell_type":"code"},"source":"# Predict which cluster each of the test images lives in of the test images\npreds = kmeans_estimator.predict(X_test_gray)","block_group":"f5637cb4d32d4ab38afa6029ad5482ba","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"w0DPWXBegiAB","colab_type":"text","cell_id":"5b0a47990b1c4effb2569c0bf96eb1d9","deepnote_cell_type":"markdown"},"source":"Although we now have the predicted cluster number for each of the test images, we need to convert those into actual classification values.","block_group":"5b0a47990b1c4effb2569c0bf96eb1d9"},{"cell_type":"code","metadata":{"id":"7GwsS8237FY8","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669434186,"execution_millis":3,"deepnote_to_be_reexecuted":true,"cell_id":"c31ec2e48c5649fd8811ded59655b73d","deepnote_cell_type":"code"},"source":"# Assign Class prediction to match IDs\npred_labels = np.zeros_like(preds)\nmask = pred_labels.copy().astype(bool)\n\n# Loop over each cluster\nfor icluster in range(8):\n    # Build a mask indicating which test images have this cluster number\n    mask = (preds == icluster)\n\n    # Grab the *most common* label associated with these test images\n    pred_labels[mask] = mode(y_test[mask]).mode[0]","block_group":"c31ec2e48c5649fd8811ded59655b73d","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"v-jCEcsTuy8l","colab_type":"text","cell_id":"99bd6454f9aa45aeb81f99088d05474f","deepnote_cell_type":"markdown"},"source":"With our final classification predictions in hand, we can proceed to compare the predictions to the *correct* answers stored in the \"y_test_gray\" variable. As a first step, we'll examine the true and predicted labels associated with a sample set of test images.","block_group":"99bd6454f9aa45aeb81f99088d05474f"},{"cell_type":"code","metadata":{"id":"7L1PgQ67u0SI","colab":{"height":657,"base_uri":"https://localhost:8080/"},"outputId":"77e39dd4-eeae-4804-ce44-73dc834e85cb","colab_type":"code","source_hash":null,"execution_start":1626669434195,"execution_millis":726,"deepnote_to_be_reexecuted":true,"cell_id":"dba86ef656794fc69a3e68e2e2b9998f","deepnote_cell_type":"code"},"source":"# Reshape the image data back into a 2D format\nnum_test = len(y_test)\nimages_test_gray = X_test_gray.reshape((num_test, nrows, ncols))\n\n# Generate some random samples\nnp.random.seed(123456789)\nsample_inds = np.random.choice(num_test, 8, replace=False)\n\n# Generate a figure\nplt.figure(figsize=(16, 11), dpi= 80)\n\n# Loop over eight images, display them and show their labels\nfor ii, ind in enumerate(sample_inds):\n    plt.subplot(2,4,ii+1)\n    plt.imshow(np.round(images_test_gray[ind]).astype(int), cmap='gray')\n    title_str = ('predicted: ' + label_to_str[pred_labels[ind]] + '\\n' +\n                 'expected: ' + label_to_str[y_test[ind]])\n    plt.title(title_str)\n    plt.axis('off')\n\nplt.show()","block_group":"dba86ef656794fc69a3e68e2e2b9998f","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"KWRP7VDalcwz","colab_type":"text","cell_id":"74ae87e0c1b140dea8593baf107c456d","deepnote_cell_type":"markdown"},"source":"As a single, summarry statistic, let's compute the total accuracy of this classifier. This can be done using the \"accuracy_score\" function imported at the beginning of this notebook.","block_group":"74ae87e0c1b140dea8593baf107c456d"},{"cell_type":"code","metadata":{"id":"MGBNe9io9YOz","colab":{"height":51,"base_uri":"https://localhost:8080/"},"outputId":"057b8e49-18da-46bf-ccae-052eecda610f","colab_type":"code","source_hash":null,"execution_start":1626669434912,"execution_millis":3019127,"deepnote_to_be_reexecuted":true,"cell_id":"aafb33891d5f42f48b1b5d4972c9e9b2","deepnote_cell_type":"code"},"source":"# Calculate accurate = (correctly labeled images /  number of images)\nacc_gray = accuracy_score(y_test, pred_labels)\nprint('k-means clustering on raw grayscale pixel values...')\nprint(f'Total accuracy: {acc_gray:4.1%}')","block_group":"aafb33891d5f42f48b1b5d4972c9e9b2","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"_A6CslYVmPNP","colab_type":"text","cell_id":"c771ae78f51b4406926a2048eddbe3c4","deepnote_cell_type":"markdown"},"source":"Unless the random number generators behind the k-means algorithm were extremely kind to you, the total accuracy of this algorithm probably does not look very promising at this point. Nevertheless, lets see if we can find a different set of features that may work better with the k-means algorithm.\n\n## Pixel color values\nWe averaged the RGB channel values when loading the dataset above to get a set of grayscale images. That averaging procedure threw away a lot of information, and that information may be helpful in classifying these images. For example, you may notice some color related patterns in the example histological images at the top of this notebook. The images for class \"a\" are rather red while the images for classes \"g\" and \"h\" have very little red color in them.\n\nNext, we will try re-running the k-means algorithm, but this time we will keep all of the RGB channels.","block_group":"c771ae78f51b4406926a2048eddbe3c4"},{"cell_type":"code","metadata":{"id":"VF5XR2TGKfTe","colab":{"height":221,"base_uri":"https://localhost:8080/"},"outputId":"0977a1a5-0f6c-4091-8533-e07afa976430","colab_type":"code","source_hash":null,"execution_start":1626669434912,"execution_millis":42533,"deepnote_to_be_reexecuted":true,"cell_id":"4a87ef72d77340f590117f826d807eb1","deepnote_cell_type":"code"},"source":"# Clear out all of the old copies of the data for memory management\ndel X_train_gray, X_test_gray, y_train, y_test, images_test_gray\n\n# Load the data using our data-loading function\nimages_lab, labels, label_to_str = load_images(colorspace='lab')","block_group":"4a87ef72d77340f590117f826d807eb1","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"onNXBhoZs1uw","colab_type":"text","cell_id":"64b9927a91e444f89ad98dc394823cef","deepnote_cell_type":"markdown"},"source":"Now that we have loaded these images and converted them into L* a* b* space, we can really focus in on the *color* information. To do that, we are going to *discard* the brightness information, which is fairly well isolated in the L* component of the images.","block_group":"64b9927a91e444f89ad98dc394823cef"},{"cell_type":"code","metadata":{"id":"siHH5B0eNyju","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669477445,"execution_millis":1,"deepnote_to_be_reexecuted":true,"cell_id":"6cdbc9ef337044fd9a1b51c0454acd6a","deepnote_cell_type":"code"},"source":"# Taking out the L* channel, leaving only a* and b* channels\nimages_ab = images_lab[:,:,:,1:3]\ndel images_lab","block_group":"6cdbc9ef337044fd9a1b51c0454acd6a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"HeoCYvzot1ku","colab_type":"text","cell_id":"946441034b2f4ee38ea02b6236943dc3","deepnote_cell_type":"markdown"},"source":"We again need to flatten these images into vectors, and the code in this cell does that for us.","block_group":"946441034b2f4ee38ea02b6236943dc3"},{"cell_type":"code","metadata":{"id":"b-KfwMDFOEvh","colab":{"height":34,"base_uri":"https://localhost:8080/"},"outputId":"0a8222ed-e8b5-4e0e-8af8-143860a201ba","colab_type":"code","source_hash":null,"execution_start":1626669477449,"execution_millis":914,"deepnote_to_be_reexecuted":true,"cell_id":"e8ed745dcc9b49058564627a1fb8ce4f","deepnote_cell_type":"code"},"source":"# Flattening the images into vectors\nnum_images, nrows, ncols, ndims = images_ab.shape\nimages_ab = images_ab.reshape((num_images, nrows*ncols*ndims))\nprint('{} images flattened into vectors of length {}'.format(*images_ab.shape))","block_group":"e8ed745dcc9b49058564627a1fb8ce4f","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"HyBJdehypqVR","colab_type":"text","cell_id":"d09a7157c7ab47fba0c18cf40a63192c","deepnote_cell_type":"markdown"},"source":"We will again split this data into training and test sets.","block_group":"d09a7157c7ab47fba0c18cf40a63192c"},{"cell_type":"code","metadata":{"id":"tEWAXVQ8Mp3b","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669478360,"execution_millis":511,"deepnote_to_be_reexecuted":true,"cell_id":"e31e0f5f6a4744eaad249ac919035a4f","deepnote_cell_type":"code"},"source":"# Split the data into training and testing sets\n# NOTE: using convention of \"X\" as data and \"y\" as label\ntmp = train_test_split(np.arange(num_images), images_ab, labels, test_size=0.25)\ninds_train, inds_test, X_train_ab, X_test_ab, y_train, y_test = tmp\n\n# We no longer need a copy of the \"images_ab\" or \"tmp\" variables\ndel images_ab\ndel tmp","block_group":"e31e0f5f6a4744eaad249ac919035a4f","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"2vsvfm1rw84Q","colab_type":"text","cell_id":"8dcad7e86cec4e979ddf89379d779c43","deepnote_cell_type":"markdown"},"source":"Now repeat the k-means clusterin algorithm using the data from the a* and b* color channels of the images.","block_group":"8dcad7e86cec4e979ddf89379d779c43"},{"cell_type":"code","metadata":{"id":"KE32SGc4QPFB","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669478874,"execution_millis":44247,"deepnote_to_be_reexecuted":true,"cell_id":"1317ccf1ef454eb5b708ecce100a0335","deepnote_cell_type":"code"},"source":"# Cluster the images using the KMeans algorithm\nkmeans_estimator = KMeans(n_clusters=8, max_iter=10).fit(X_train_ab)","block_group":"1317ccf1ef454eb5b708ecce100a0335","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"3eOYnaECUp0Z","colab_type":"text","cell_id":"50457519599f4898b20332ca04c354c1","deepnote_cell_type":"markdown"},"source":"Predict which cluster each of the test images is associated with.","block_group":"50457519599f4898b20332ca04c354c1"},{"cell_type":"code","metadata":{"id":"kCHAt88IUlHi","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669523166,"execution_millis":336,"deepnote_to_be_reexecuted":true,"cell_id":"9b92573b13704ee7b4ff2d46a91722b3","deepnote_cell_type":"code"},"source":"# Make predictions for the test images based on the clustering\npreds = kmeans_estimator.predict(X_test_ab)","block_group":"9b92573b13704ee7b4ff2d46a91722b3","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"d40srwPORAjG","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669523505,"execution_millis":4,"deepnote_to_be_reexecuted":true,"cell_id":"bfba6f73fb4e47dbb279c00f227696a7","deepnote_cell_type":"code"},"source":"# Assign Class prediction to match IDs\npred_labels = np.zeros_like(preds)\nmask = pred_labels.astype(bool)\n\n# Loop over each cluster\nfor icluster in range(8):\n    # Build a mask indicating which test images have this cluster number\n    mask = (preds == icluster)\n\n    # Grab the *most common* label associated with these test images\n    pred_labels[mask] = mode(y_test[mask]).mode[0]","block_group":"bfba6f73fb4e47dbb279c00f227696a7","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"5VwwWcbCxk1O","colab_type":"text","cell_id":"4cfbe915a7f240fcbdc463e44603153c","deepnote_cell_type":"markdown"},"source":"With our final classification predictions in hand, we can proceed to compare the predictions to the *correct* answers stored in the \"y_test\" variable. As a first step, we'll examine the true and predicted labels associated with a sample set of test images.","block_group":"4cfbe915a7f240fcbdc463e44603153c"},{"cell_type":"code","metadata":{"id":"9eqUmNKqxk1e","colab":{"height":888,"base_uri":"https://localhost:8080/"},"outputId":"ec117836-e924-4252-de1c-052af5e37653","colab_type":"code","source_hash":null,"execution_start":1626669523512,"execution_millis":2497,"deepnote_to_be_reexecuted":true,"cell_id":"16f672c2cb2140a4a0009fc439ccd5d6","deepnote_cell_type":"code"},"source":"# Load the RGB versions of the images\nimages_rgb, _, _ = load_images(colorspace='rgb')\n\n# Generate some random samples\nnum_test = len(y_test)\nnp.random.seed(123456789)\nsample_inds = np.random.choice(num_test, 8, replace=False)\n\n# Generate a figure\nplt.figure(figsize=(18, 11), dpi= 80)\n\n# Loop over eight images, display them and show their labels\nfor ii, ind  in enumerate(sample_inds):\n    plt.subplot(2,4,ii+1)\n    plt.imshow(np.round(images_rgb[inds_test[ind]]).astype(int))\n    title_str = ('predicted: ' + label_to_str[pred_labels[ind]] + '\\n' +\n                 'expected: ' + label_to_str[y_test[ind]])\n    plt.title(title_str)\n    plt.axis('off')\n\nplt.show()","block_group":"16f672c2cb2140a4a0009fc439ccd5d6","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"C-10FwWFxjv-","colab_type":"text","cell_id":"15d87abae55b49ca9bedc0158da6443d","deepnote_cell_type":"markdown"},"source":"As a summary statistic, we compute the total accuracy of the k-means clustering algorithm on the a* and b* color channels.","block_group":"15d87abae55b49ca9bedc0158da6443d"},{"cell_type":"code","metadata":{"id":"UJbE36HvRGAV","colab":{"height":51,"base_uri":"https://localhost:8080/"},"outputId":"bf3a2750-d62f-4213-985b-88eae3b67ba8","colab_type":"code","source_hash":null,"execution_start":1626669526002,"deepnote_to_be_reexecuted":true,"cell_id":"5f61e68f791f4edf8d6672bbd97c7bae","deepnote_cell_type":"code"},"source":"# Calculate Accuracy\nacc = accuracy_score(y_test, pred_labels)\nprint('KMeans clustering on raw a* and b* pixel values...')\nprint(f'Total accuracy: {acc:4.1%}')","block_group":"5f61e68f791f4edf8d6672bbd97c7bae","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"luTWYfJ58Fzc","colab_type":"text","cell_id":"4c6d7fae45fd4fd2826ebcbd4f1a7bbc","deepnote_cell_type":"markdown"},"source":"It seems that our intuation may have been a bit misguieded, as it does not appear to be the case that using the a* and b* color information significantly improved the classification accuracy. In fact, it seems to have reduced the classification accuracy!\n\nIn general, it seems that using raw image data as a feature set is not a particularly great way to classify images. In the next section, we will consider how we might generate some more thoughtfully engineered features for image classification.","block_group":"4c6d7fae45fd4fd2826ebcbd4f1a7bbc"},{"cell_type":"markdown","metadata":{"id":"Ui4vX6azllTH","colab_type":"text","cell_id":"03a92ca3dc4146b0a6d11d101d630254","deepnote_cell_type":"markdown"},"source":"## The Co-occurrence Matrix\nOne option for extracting features from an image is to use the gray-level co-occurance matrix. This matrix captures information about the relationships between pixel brightnesses (gray-level) and pixel offsets and is a common tool for extracting information about texture in an image. For an $n \\times m$ image $I$ and a pre-specified offset $(\\Delta x, \\Delta y)$, the co-occurance matrix $P_{i,j}$ is defined as the number of times that gray-level $j$ occurs at an offset $(\\Delta x, \\Delta y)$ from gray-level $i$. For the equation, see https://en.wikipedia.org/wiki/Co-occurrence_matrix. \n\n<!-- $$P_{i,j} = \\sum_{x=1}^{n} \\sum_{y=1}^{m}\n\\begin{cases}\n    1, & \\text{if $\\quad I(x, y) = i \\quad$ and $\\quad I(x + \\Delta x, y + \\Delta y) = j$} \\\\\n    0, & \\text{otherwise}\n\\end{cases}$$ -->\n\nNotes on the equation: In the article, the Co-occurence matrix is referred to as $C_{i,j}$, but we will generally call it $P_{i,j}$. $i$ and $j$ can be any value from $0$ up to the maximum pixel value in the image. Thus, $P_{i,j}$ is a square matrix with shape ($n_{\\rm level} \\times n_{\\rm level}$).\n\nNotice how this matrix is defined for a particular pixel offset  $(\\Delta x, \\Delta y)$. Thus, we can compute several co-occurance matrices corresponding to different pixel offsets. Each of these matrices will capture information about the texture of an image at particular scales and directions. An example co-occurence matrix for an offset ($\\Delta x = 1, \\Delta y = 0$) is given below.\n\n![Example co-occurrence matrix](https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/Images/Week3/co-occurrence-matrix_full.png)\n","block_group":"03a92ca3dc4146b0a6d11d101d630254"},{"cell_type":"markdown","metadata":{"id":"mpL4A0JcSODX","colab_type":"text","cell_id":"09bdf4b2f4954e078a91e1ff91834624","deepnote_cell_type":"markdown"},"source":"With the co-occurance matrices computed, we can extract several features from them. For reference, the features computed from the co-occurance matrix ($P_{i,j}$) are\n\n* contrast: $\\sum_{i,j=0}^{n_{\\rm level} - 1} P_{i,j} \\times (i-j)^2$\n\n* dissimilarity: $\\sum_{i,j=0}^{n_{\\rm level} - 1} P_{i,j} \\times \\left|i-j\\right|$\n\n* homogeneity: $\\sum_{i,j=0}^{n_{\\rm level} - 1} \\frac{P_{i,j}}{1+(x-j)^2}$\n\n* ASM: $\\sum_{i,j=0}^{n_{\\rm level} - 1} P_{i,j}^2$\n\n* energy: $\\sqrt{\\rm ASM}$\n\n* correlation: $\\sum_{i,j=0}^{n_{\\rm level} - 1} P_{i,j} \n\\times \\left[\n    \\frac{(i - \\mu_i) (j - \\mu_j)}\n    {\\sqrt{(\\sigma_i^2 \\sigma_j^2}}\n\\right]$,\n\nwhere $\\mu$ and $\\sigma$ are the weighted average and standard deviaton of $i$ and $j$ values in the $P_{i,j}$ matrix.","block_group":"09bdf4b2f4954e078a91e1ff91834624"},{"cell_type":"code","metadata":{"id":"9ayXklXe5uSj","colab":{"height":221,"base_uri":"https://localhost:8080/"},"outputId":"8772b237-6d1b-4b25-eaca-b05c1760a1c3","colab_type":"code","source_hash":null,"execution_start":1626669526003,"execution_millis":4562,"deepnote_to_be_reexecuted":true,"cell_id":"e7baa6a759194c898db3d414d1aae4a6","deepnote_cell_type":"code"},"source":"# Begin by clearing out all the old data if that has not yet been done\ntry:\n    del X_train_ab, X_test_ab, y_train, y_test\nexcept:\n    pass\ntry:\n    del images_rgb\nexcept:\n    pass\n\n# Now reload the grayscale images\nimages_gray, labels, label_to_str = load_images(colorspace='gray')\nnum_images = len(labels)","block_group":"e7baa6a759194c898db3d414d1aae4a6","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"DACt0S1huK76","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669530567,"execution_millis":45627,"deepnote_to_be_reexecuted":true,"cell_id":"846802b3301b4391ae0160b713e81f7e","deepnote_cell_type":"code"},"source":"# Initialize an array in which to store the image features\ncomatrix_features = np.array([])\n\n# Loop over each image in the training set\nfor k in range(num_images):\n  # Compute the grey-lever co-occurence matrix for this image\n  # NOTE: For some reason the scikit-image.greycomatrix function specifies pixel\n  # offsets in terms of distance and angles. Here we use a distance of 1 pixel\n  # and angular offsets of 0 and pi/2 to specify the offsets (dx = 1, dy = 0)\n  # and (dx = 0, dy = 1).\n  comatrix = greycomatrix(images_gray[k].astype('int'), distances=[1],\n                        angles=[0, np.pi/2], levels = 256)\n  comatrix_features = np.append(comatrix_features, greycoprops(comatrix, 'contrast'))\n  comatrix_features = np.append(comatrix_features, greycoprops(comatrix, 'dissimilarity'))\n  comatrix_features = np.append(comatrix_features, greycoprops(comatrix, 'homogeneity'))\n  comatrix_features = np.append(comatrix_features, greycoprops(comatrix, 'energy'))\n  comatrix_features = np.append(comatrix_features, greycoprops(comatrix, 'correlation'))\n  comatrix_features = np.append(comatrix_features, greycoprops(comatrix, 'ASM'))\n\n# Reshape the array of features so that first axis is the image index and second\n# axis is the feature index.\ncomatrix_features = comatrix_features.reshape(num_images, 12)","block_group":"846802b3301b4391ae0160b713e81f7e","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"gmn44wpQ63gs","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669576197,"execution_millis":1,"deepnote_to_be_reexecuted":true,"cell_id":"28cb12fcf44e401d9eb76e30373395c2","deepnote_cell_type":"code"},"source":"# Split the data into training and testing sets\n# NOTE: using convention of \"X\" as data/features and \"y\" as label\ntmp = train_test_split(np.arange(num_images), comatrix_features, labels, test_size=0.25)\ninds_train, inds_test, X_train_comatrix, X_test_comatrix, y_train, y_test = tmp","block_group":"28cb12fcf44e401d9eb76e30373395c2","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"mfzVMURoS_IW","colab_type":"text","cell_id":"3c3ce731c77a4e9caf3c79d9f7d99968","deepnote_cell_type":"markdown"},"source":"With those features extracted from the co-occurence matrices for each of the training and test images, we proceed to apply k-means clustering to these features.","block_group":"3c3ce731c77a4e9caf3c79d9f7d99968"},{"cell_type":"code","metadata":{"id":"fQJ1DtSU8fvX","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669576201,"execution_millis":869,"deepnote_to_be_reexecuted":true,"cell_id":"c9d1d097986b421cadc07db2fbe3319d","deepnote_cell_type":"code"},"source":"# Cluster the images using the KMeans algorithm\nkmeans_estimator = KMeans(n_clusters=8, max_iter=10).fit(X_train_comatrix)","block_group":"c9d1d097986b421cadc07db2fbe3319d","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"wfFHzg6y8fvi","colab_type":"text","cell_id":"982faf0a25704dc0b1768f0a9628ac8c","deepnote_cell_type":"markdown"},"source":"Predict which cluster each of the test images is associated with.","block_group":"982faf0a25704dc0b1768f0a9628ac8c"},{"cell_type":"code","metadata":{"id":"WknyN37K8fvk","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669577071,"execution_millis":3,"deepnote_to_be_reexecuted":true,"cell_id":"43658c2e622b4ab59de963f85796150b","deepnote_cell_type":"code"},"source":"# Make predictions for the test images based on the clustering\npreds = kmeans_estimator.predict(X_test_comatrix)","block_group":"43658c2e622b4ab59de963f85796150b","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"j7nvdnyD8fvu","colab":{},"colab_type":"code","source_hash":null,"execution_start":1626669577078,"execution_millis":80,"deepnote_to_be_reexecuted":true,"cell_id":"68b568842e9741a5a691a238daf1bbf0","deepnote_cell_type":"code"},"source":"# Assign Class prediction to match IDs\npred_labels = np.zeros_like(preds)\nmask = pred_labels.astype(bool)\n\n# Loop over each cluster\nfor icluster in range(8):\n    # Build a mask indicating which test images have this cluster number\n    mask = (preds == icluster)\n\n    # Grab the *most common* label associated with these test images\n    pred_labels[mask] = mode(y_test[mask]).mode[0]","block_group":"68b568842e9741a5a691a238daf1bbf0","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"LPGmpGrY8fv4","colab_type":"text","cell_id":"88dae48c4f27451996bf158809688358","deepnote_cell_type":"markdown"},"source":"With our final classification predictions in hand, we can proceed to compare the predictions to the *correct* answers stored in the \"y_test\" variable. As a first step, we'll examine the true and predicted labels associated with a sample set of test images.","block_group":"88dae48c4f27451996bf158809688358"},{"cell_type":"code","metadata":{"id":"AZHU4eMullTS","colab":{"height":888,"base_uri":"https://localhost:8080/"},"outputId":"21115c9a-c9ac-46d5-bfaa-c1cb35e4102d","colab_type":"code","source_hash":null,"execution_start":1626669577162,"execution_millis":2386,"deepnote_to_be_reexecuted":true,"cell_id":"869df31c492c4981876aeae69a423890","deepnote_cell_type":"code"},"source":"# Load the RGB versions of the images\nimages_rgb, _, _ = load_images(colorspace='rgb')\n\n# Generate some random samples\nnum_test = len(y_test)\nnp.random.seed(123456789)\nsample_inds = np.random.choice(num_test, 8, replace=False)\n\n# Generate a figure\nplt.figure(figsize=(18, 11), dpi= 80)\n\n# Loop over eight images, display them and show their labels\nfor ii, ind  in enumerate(sample_inds):\n    plt.subplot(2,4,ii+1)\n    plt.imshow(np.round(images_rgb[inds_test[ind]]).astype(int))\n    title_str = ('predicted: ' + label_to_str[pred_labels[ind]] + '\\n' +\n                 'expected: ' + label_to_str[y_test[ind]])\n    plt.title(title_str)\n    plt.axis('off')\n\nplt.show()","block_group":"869df31c492c4981876aeae69a423890","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"ut6M4Ib09Qjf","colab_type":"text","cell_id":"9f6936c584d94c24a8753bf06163f8c5","deepnote_cell_type":"markdown"},"source":"As a summary statistic, we compute the total accuracy of the k-means clustering algorithm on the gray-level co-occurence matrix features.","block_group":"9f6936c584d94c24a8753bf06163f8c5"},{"cell_type":"code","metadata":{"id":"d9wZP5mvTLIL","colab":{"height":170,"base_uri":"https://localhost:8080/"},"outputId":"8ab8f3b9-ba38-4e1c-deea-94a819622e8e","colab_type":"code","source_hash":null,"execution_start":1626669579542,"deepnote_to_be_reexecuted":true,"cell_id":"bbe437753b8b46dc8a44f7453add29c3","deepnote_cell_type":"code"},"source":"# Get the accuracy and confusion matrix\nacc = accuracy_score(y_test, pred_labels)\nmat = confusion_matrix(y_test, pred_labels)\nprint(acc)\nprint(mat)","block_group":"bbe437753b8b46dc8a44f7453add29c3","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"JfzhkB_DllTl","colab":{"height":398,"base_uri":"https://localhost:8080/"},"outputId":"4f2020ce-9fe6-4818-aca8-20b01d3c74b3","colab_type":"code","source_hash":null,"execution_start":1626669579543,"execution_millis":209,"deepnote_to_be_reexecuted":true,"cell_id":"1073b65b6c004d5ca6d6b4be88defd97","deepnote_cell_type":"code"},"source":"# Also visualize the confusion matrix\nplt.figure(figsize=(8,6))\nplt.imshow(mat, cmap='hot', interpolation='nearest')\nplt.grid(False)\nplt.colorbar()\nplt.xlabel('True label')\nplt.ylabel('Predicted label')\nplt.show()","block_group":"1073b65b6c004d5ca6d6b4be88defd97","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=37179657-af6c-4b10-ba18-5b7441fab104' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"8e5be6af78f24ac7a1906b36eafe6949"}}