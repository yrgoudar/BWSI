{"cells":[{"cell_type":"markdown","metadata":{"id":"a7scg4EGMfYk","colab_type":"text","cell_id":"052d64a4c5eb4eaf9d321362a9991871","deepnote_cell_type":"markdown"},"source":"# Building a 2D CNN for Image Classification\nIn this tutorial, we will be learning how to build a 2D version of a Convolutional Neural Network (CNN).\n\n\nFor this example, we will again be using the same dataset of histological images: http://dx.doi.org/10.5281/zenodo.53169. \n\n![Example histological images for classes a through f](https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/Images/Week3/Representative-images-the-first-10-images-of-every-tissue-class.png?token=ADGRCSE2YIYWUNCJTHNB5XK67T4HE)","block_group":"052d64a4c5eb4eaf9d321362a9991871"},{"cell_type":"markdown","metadata":{"id":"GESM76HY9Dea","colab_type":"text","cell_id":"724f26c5f43c4be9ae53c6d874d4b459","deepnote_cell_type":"markdown"},"source":"# Imports\nStart by importing the packages and modules we will be needing for this project.","block_group":"724f26c5f43c4be9ae53c6d874d4b459"},{"cell_type":"code","metadata":{"id":"W86Oi6dCMfYp","colab":{},"colab_type":"code","source_hash":"19901d6c","execution_start":1626671575678,"execution_millis":4397,"deepnote_to_be_reexecuted":false,"cell_id":"98769646fbca469880a512cff98c155a","deepnote_cell_type":"code"},"source":"# Basic operating system (os), numerical, and plotting functionality\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# scikit-learn data utilities\nfrom sklearn.model_selection import train_test_split\nimport skimage.transform as image_transform\n\n# Color transformations\nfrom skimage.color import rgb2lab\n\n# scikit-learn performance metric utilities\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# Import our neural network building tools\nimport tensorflow as tf\n\n# Garbage collection (for saving RAM during training)\nimport gc","block_group":"98769646fbca469880a512cff98c155a","execution_count":null,"outputs":[{"name":"stderr","text":"2021-07-19 05:12:57.105598: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2021-07-19 05:12:57.105637: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/d4da3568-bccf-4982-8159-8d49f4c734a8","content_dependencies":null},{"cell_type":"code","metadata":{"id":"m2wfqw5B2SPc","colab":{},"colab_type":"code","source_hash":"46fdeac7","execution_start":1626671580076,"execution_millis":0,"deepnote_to_be_reexecuted":false,"cell_id":"a5afdea88aed40209d57b7a3fea90211","deepnote_cell_type":"code"},"source":"# Set plotting preferences\nimport matplotlib\n%matplotlib inline\nfont = {'family' : 'sans-serif',\n        'weight' : 'normal',\n        'size'   : 16}\nmatplotlib.rc('font', **font)","block_group":"a5afdea88aed40209d57b7a3fea90211","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"Ta7q5xwIllSi","colab_type":"text","cell_id":"0f106bc1482341d18da142d9db1aa9ef","deepnote_cell_type":"markdown"},"source":"# Retrieve and Load the Data\nThe following cell checks if the histological images have already been downloaded into the current Colab session and downloads them if not.","block_group":"0f106bc1482341d18da142d9db1aa9ef"},{"cell_type":"code","metadata":{"id":"KJvH96N6J4tP","colab":{"height":102,"base_uri":"https://localhost:8080/"},"outputId":"edf0df08-ee66-4efc-9593-e85abdeba457","colab_type":"code","source_hash":"a57636d5","execution_start":1626671580077,"execution_millis":0,"deepnote_to_be_reexecuted":false,"cell_id":"d01ea5d55a714ab082a6f6ac82e1cb04","deepnote_cell_type":"code"},"source":"# Define the current directory and the directory where the files to download can\n# be found\ncurrent_dir = '/'\nremote_path = 'https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/'\n\n# Define and build a directory to save this data in\ndata_dir = '/crc_data'\nif not os.path.isdir(data_dir):\n  os.mkdir(data_dir)\n\n# Move into the data directory and download all of the files\nos.chdir(data_dir)\nfor ii in range(1, 6):\n    basename = f'rgb0{ii}.npz'\n    filename = os.path.join(remote_path, basename)\n\n    # Check if the file has already been downloaded\n    if not os.path.isfile(basename):\n      cmd = f'wget {filename}'\n      print(cmd)\n      os.system(cmd)\n\n# Return to the original directory\nos.chdir(current_dir)","block_group":"d01ea5d55a714ab082a6f6ac82e1cb04","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"tDGLNYFqJ90l","colab_type":"text","cell_id":"13496735b7f642d3af75faba6d8117bf","deepnote_cell_type":"markdown"},"source":"The downloaded \".npz\" archives can now be read into memory and parsed using our data loading function copied from the previous tutorial.","block_group":"13496735b7f642d3af75faba6d8117bf"},{"cell_type":"code","metadata":{"id":"Y9rC7eJMllSv","colab":{},"colab_type":"code","source_hash":"18619457","execution_start":1626671580078,"execution_millis":40,"deepnote_to_be_reexecuted":false,"cell_id":"0a754e13974a4cf9b40b4381e364fe6a","deepnote_cell_type":"code"},"source":"# Define a function to load the data from the assumed download path\ndef load_images(colorspace='rgb'):\n    \"\"\"\n    Loads the example data and applies transformation into requested colorspace\n\n    Arguments\n    ---------\n    colorspace : str, optional, default: `rgb`\n        The colorspace into which the images should be transformed. Accepted\n        values include\n\n        'rgb' : Standard red-green-blue color-space for digital images\n\n        'gray' or 'grey': An arithmetic average of the (r, g, b) values\n\n        'lab': The CIE L* a* b* colorspace\n    \n    Returns\n    -------\n    images : numpy.ndarray, shape (Nimg, Ny, Nx, Ncolor)\n        The complete set of transformed images\n\n    labels : numpy.ndarray, shape (Nimg)\n        The classification labels associated with each entry in `images`\n\n    label_to_str : dict\n        A dictionary which converts the numerical classification value in\n        `labels` into its string equivalent representation.\n    \"\"\"\n    # Check that the colorspace argument is recognized\n    colorspace_lower = colorspace.lower()\n    if colorspace_lower not in ['rgb', 'gray', 'grey', 'lab']:\n        raise ValueError(f'`colorspace` value of {colorspace} not recognized')\n\n    # Load data, which is stored as a numpy archive file (.npz)\n    filename = os.path.join(data_dir, 'rgb01.npz')\n    print(f'loading {filename}')\n    tmp = np.load(os.path.join(data_dir, 'rgb01.npz'), allow_pickle=True)\n\n    # Parse the loaded data into images and labels\n    # Initialize the images and labels variables using the first archive data\n    images = tmp['rgb_data']\n    if colorspace_lower == 'rgb':\n        pass\n    elif colorspace_lower in ['gray', 'grey']:\n        images = np.mean(images, axis=-1)      # Average into grayscale\n    elif colorspace_lower == 'lab':\n        images = rgb2lab(images)               # Convert to CIE L*a*b*\n\n    # Grab the initial array for the image labels\n    labels = tmp['labels']\n    \n    # Grab the dictionary to convert numerical labels to their string equivalent\n    label_to_str = tmp['label_str']\n    label_to_str = label_to_str.tolist() # Convert label_to_str into a dict\n\n    # Update the user on the number and size of images loaded\n    print('Loaded images with shape {}'.format(images.shape))\n    del tmp\n\n    # Loop over each of the remaining archives and append the contained data\n    for ii in range(2,6):\n        # Build the full path to the archive and load it into memory\n        filename = os.path.join(data_dir, f'rgb0{ii}.npz')\n        print(f'loading {filename}')\n        tmp = np.load(filename, allow_pickle=True)\n\n        # Parse and append the data\n        these_images = tmp['rgb_data']\n        if colorspace_lower == 'rgb':\n            pass\n        elif (colorspace_lower == 'gray') or (colorspace_lower == 'grey'):\n            these_images = np.mean(these_images, axis=-1) # Convert to grayscale\n        elif colorspace_lower == 'lab':\n            these_images = rgb2lab(these_images)          # Convert to CIEL*a*b*\n\n        # Append the images and labels\n        images = np.append(images, these_images, axis=0)\n        labels = np.append(labels, tmp['labels'], axis=0)\n\n        # Update the user on the number and size of images\n        print('Loaded images with shape {}'.format(these_images.shape))\n        del tmp\n\n    # Force the image data to be floating point and print the data shape\n    images = images.astype(np.float)\n    print('Final image data shape: {}'.format(images.shape))\n    print('Number of image labels: {}'.format(*labels.shape))\n\n    return images, labels, label_to_str","block_group":"0a754e13974a4cf9b40b4381e364fe6a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"1LGPaPqqrSoQ","colab_type":"text","cell_id":"f4be100a6e4e4e258055e679d37fa2ae","deepnote_cell_type":"markdown"},"source":"With this data-loading function defined, let's load in the grayscale images.","block_group":"f4be100a6e4e4e258055e679d37fa2ae"},{"cell_type":"code","metadata":{"id":"NsJAjmJgM1Od","colab":{"height":221,"base_uri":"https://localhost:8080/"},"outputId":"53531be4-a729-44b4-81ce-6727ff50adda","colab_type":"code","source_hash":"4cb058b","execution_start":1626671580118,"execution_millis":4481,"deepnote_to_be_reexecuted":false,"cell_id":"7e6c3b7835a6420394e425809df70648","deepnote_cell_type":"code"},"source":"# Load the data using our data-loading function\n#\n# You can change the `colorspace` keyword argument in `load_images` function\n# call to try to try training classifier an image in another colorspace\nimages_full_res, labels, label_to_str = load_images(colorspace='gray')\nnum_classes = np.unique(labels).size","block_group":"7e6c3b7835a6420394e425809df70648","execution_count":null,"outputs":[{"name":"stdout","text":"loading /crc_data/rgb01.npz\nLoaded images with shape (1000, 150, 150)\nloading /crc_data/rgb02.npz\nLoaded images with shape (1000, 150, 150)\nloading /crc_data/rgb03.npz\nLoaded images with shape (1000, 150, 150)\nloading /crc_data/rgb04.npz\nLoaded images with shape (1000, 150, 150)\nloading /crc_data/rgb05.npz\nLoaded images with shape (1000, 150, 150)\nFinal image data shape: (5000, 150, 150)\nNumber of image labels: 5000\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/91cdbb97-75da-43ca-bd47-e34de2eee5ac","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"-1_mJOsLM5J-","colab_type":"text","cell_id":"cae78382be3d46f2a664ed945536ad55","deepnote_cell_type":"markdown"},"source":"# Pre-process the Images\n\nThe following cells apply some pre-processing steps to the image data.","block_group":"cae78382be3d46f2a664ed945536ad55"},{"cell_type":"markdown","metadata":{"id":"Cy0cWNN3aOGa","colab_type":"text","cell_id":"35aa063e26834cc0a922496e4250954f","deepnote_cell_type":"markdown"},"source":"## Resize the Images for Faster Training\n\nIn the following cell, we resize the images so that they are lower resolution than the originals. This only has a minimal impact on the classifier performance, but it significantly accelerates training.","block_group":"35aa063e26834cc0a922496e4250954f"},{"cell_type":"code","metadata":{"id":"U3ue6DZ6M23D","colab":{},"colab_type":"code","source_hash":"c40c37ab","execution_start":1626671584602,"execution_millis":5860,"deepnote_to_be_reexecuted":false,"cell_id":"fdd61716c7624336b990c22031e73747","deepnote_cell_type":"code"},"source":"# Grab the original shape of the images\nnum_images = images_full_res.shape[0]\n\n# Specify a new shape to use for the resized images\n# NOTE: This can be modified to trade-off between training-speed and performance\noriginal_shape = images_full_res.shape\nnew_shape = list(original_shape)\nnew_shape[1:3] = (48, 48)\n\n# Initialize an array for storing the resized images\nimages = np.zeros(new_shape)\n\n# Loop over each image in the data and perform a resizing operation\nfor img_num, img_data in enumerate(images_full_res):\n    images[img_num] = image_transform.resize(img_data, new_shape[1:])\n\n# Remove the full-resolution versions from memory (just glogging things up)\ndel images_full_res","block_group":"fdd61716c7624336b990c22031e73747","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"PtlCeN4FMfZD","colab_type":"text","cell_id":"548d3ffaf2f34d6794244d02d78ff963","deepnote_cell_type":"markdown"},"source":"## Normalize the Image Data\nAll images should be normalized to the range $\\left[0, 1\\right]$. The procedure for this will depend on the current colorspace of the images, but some basic guidelines follow:\n\n* **RGB and greyscale:** images are often represented with integers in the range $\\left[0, 255\\right]$, so dividing by $255$ will often normalize these images (if they are not already normalized).\n\n* **HSV, HSI, HSL:** the Hue (H) channel will typically be represnted by a number in the range $\\left[0^{\\circ}, 360^{\\circ}\\right)$. The Saturation channel channels will often be represented on a scale from $\\left[0, 1\\right]$, in which case no normalization is required. The final channel (Value, Intensity, or Lightness) is often represented by a snumber in the range of either $\\left[0, 1\\right]$ or $\\left[0, 100\\right]$, so a simple investigation should indicate what normalization may be needed for that channel.\n\n* **CIE L\\*a\\*b\\*:** The L\\* channel often has a range of $\\left[0, 100\\right]$ while the a\\* and b\\* channels are in the range $\\left[-100, +100\\right]$. In some cases, the data may be represented by signed 8-bit integers, in which case the range of values is acutally maped to the discrete space from $\\left[-128, +127\\right]$, so you may need to examine if your data is integer or float.","block_group":"548d3ffaf2f34d6794244d02d78ff963"},{"cell_type":"code","metadata":{"id":"_FeL1EP5MfZF","colab":{},"colab_type":"code","source_hash":"a1aba04","execution_start":1626671590464,"execution_millis":36,"deepnote_to_be_reexecuted":false,"cell_id":"19307529f8e84902b508a50d22093fbc","deepnote_cell_type":"code"},"source":"# In this notebook, we are only considering greyscale and RGB data, so we\n# normalize by dividing by 255. If you transform the images into a different \n# colorspace, you will need to edit this code to properly normalize the data.\nif images.max() > 1:\n    images = images.astype(np.float32)/255.0","block_group":"19307529f8e84902b508a50d22093fbc","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"n5rNnXfmQwRB","colab_type":"text","cell_id":"b640a55b64cd48129610d25fb9185028","deepnote_cell_type":"markdown"},"source":"## Include an Axis for Color Channels\nIn the case where we are operating on grayscale data, there should be an axis at the end of the array of length 1 to match up with TensorFlow expectations. Otherwise, you may get an error about not having the right number of dimensions for input data.","block_group":"b640a55b64cd48129610d25fb9185028"},{"cell_type":"code","metadata":{"id":"_v912cBGrR4D","colab":{"height":68,"base_uri":"https://localhost:8080/"},"outputId":"ed13d832-7192-4ab0-cf6e-7dd1f8a6addd","colab_type":"code","source_hash":"3030017","execution_start":1626671590502,"execution_millis":11,"deepnote_to_be_reexecuted":false,"cell_id":"115c5172fc2b42008d1f0a8c6e08a0b0","deepnote_cell_type":"code"},"source":"# Take note of number of color channels in the loaded image add a last axis to \n# images ndarray if array dimension is only 3 (as is the case with grayscale images)\nif images.ndim == 3:\n    n_channels = 1\n    # If image is grayscale, then we add a last axis (of len 1) for channel\n    images = images[:, : , :, np.newaxis]\n    print('\\nlast dimension added to images ndarray to account for channel')\n    print(f'new images.shape: {images.shape}')\nelse:\n    #if image is not grayscale, last dimension of image already corresponds to channel\n    n_channels = images.shape[-1]","block_group":"115c5172fc2b42008d1f0a8c6e08a0b0","execution_count":null,"outputs":[{"name":"stdout","text":"\nlast dimension added to images ndarray to account for channel\nnew images.shape: (5000, 48, 48, 1)\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/e1388029-d485-4844-a07a-b3a230c53d89","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"QIgUR8ckRtUO","colab_type":"text","cell_id":"2d12eb13e4b04c02940a9619c8bde998","deepnote_cell_type":"markdown"},"source":"## Split the Image and Label Data\nNow we can split our pre-processed data into a training and test set.","block_group":"2d12eb13e4b04c02940a9619c8bde998"},{"cell_type":"code","metadata":{"id":"EnTT4tyXMfY9","colab":{"height":85,"base_uri":"https://localhost:8080/"},"outputId":"37e74dda-6c3e-4a54-fd06-d2f0df7a8018","colab_type":"code","source_hash":"6f6fc6a","execution_start":1626671590511,"execution_millis":18,"deepnote_to_be_reexecuted":false,"cell_id":"89e2055d65b94b4fb1f1b19c7c0c6c5e","deepnote_cell_type":"code"},"source":"# Split into training and testing sets\ntmp = train_test_split(images, labels, test_size = 0.2)\ntrain_images, test_images, train_labels, test_labels = tmp\n\n# Convert the labels from 1-D arrays to categorical type (one-hot encoding)\ntrain_labels = tf.keras.utils.to_categorical(train_labels)\ntest_labels = tf.keras.utils.to_categorical(test_labels)\n\n# Print sizes of train/test sets\nprint(f'train_images.shape: {train_images.shape}')\nprint(f'train_labels.shape: {train_labels.shape}')\nprint(f'test_images.shape: {test_images.shape}')\nprint(f'test_labels.shape: {test_labels.shape}')","block_group":"89e2055d65b94b4fb1f1b19c7c0c6c5e","execution_count":null,"outputs":[{"name":"stdout","text":"train_images.shape: (4000, 48, 48, 1)\ntrain_labels.shape: (4000, 8)\ntest_images.shape: (1000, 48, 48, 1)\ntest_labels.shape: (1000, 8)\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/34d1a5e7-cdff-4539-8f72-50d272e002e3","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"QAgbBOzBMfZI","colab_type":"text","cell_id":"592952a486114544bb68ba765d0161b8","deepnote_cell_type":"markdown"},"source":"# Building Your CNN\nAt this point, the data has been split into training and testing sets and normalized. We will now design a fully connected neural network for texture classification. \n\n\n![Example of a CNN Diagram](https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/Images/Week3/CNN-example-block-diagram.jpg)\n\n\n(Image originally from http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/ )\n\nWhen designing a fully connected network for classification, we have several decisions to make.\n\n**Network Architecuture**\n* How many layers will our network have ?\n* How many convolutional filters per layer ?\n    * What is an appropriate filter size ? \n* What is an appropriate batch size, learning rate and number of training epochs ?\n\n**Data input**\n* Do we use the raw data ?\n    * RGB or just gray channel ?\n* Does the use of different colorspaces lead to better results for a given network architecture ?\n* Can we use any of the texture features from the previous lab as inputs to this model ?\n* How does data augmentation affect the results ? \n\nOther considerations, we will not be exploring :\n* What is the trade-off between input data sizes and batch size ?\n* Is the GPU always the appropriate platform for training ?\n* How does hardware influence inputs and batch sizes for a given desired accuracy ?","block_group":"592952a486114544bb68ba765d0161b8"},{"cell_type":"code","metadata":{"id":"OsffMC7mMfZQ","colab":{"height":323,"base_uri":"https://localhost:8080/"},"outputId":"7f526405-80aa-4118-e4cc-c82b5da26515","colab_type":"code","source_hash":"5f302181","execution_start":1626671590529,"execution_millis":119,"deepnote_to_be_reexecuted":false,"cell_id":"bebc385edeae4854a86bc6af6de5ec81","deepnote_cell_type":"code"},"source":"# Create your network\n\n# Build our model sequentially\nmodel = tf.keras.Sequential()\n\n# Add input layer\nmodel.add(tf.keras.layers.Conv2D(filters=16, kernel_size=8,\n                                 padding='valid',\n                                 activation=tf.nn.relu, \n                                 input_shape=(train_images.shape[1:4])))\n\n#use 2D max pooling to reduce size of convolution output before flattening\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(10,10),\n                                       strides=3, \n                                       padding='valid',\n                                       data_format='channels_last'))\n\n#Take all activations from previous layer and flatten them \n#(often done before a fully connected layer)\nmodel.add(tf.keras.layers.Flatten())\n\n# Add fully connected layers \nmodel.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\n\n# Add final output layer - This should have as many neurons as the number\n# of classes we are trying to identify\nmodel.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax)) \n\n# Print a summary of the model\nmodel.summary()","block_group":"bebc385edeae4854a86bc6af6de5ec81","execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 41, 41, 16)        1040      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 11, 11, 16)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1936)              0         \n_________________________________________________________________\ndense (Dense)                (None, 32)                61984     \n_________________________________________________________________\ndense_1 (Dense)              (None, 8)                 264       \n=================================================================\nTotal params: 63,288\nTrainable params: 63,288\nNon-trainable params: 0\n_________________________________________________________________\n2021-07-19 05:13:10.537796: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2021-07-19 05:13:10.539949: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2021-07-19 05:13:10.539967: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2021-07-19 05:13:10.539985: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-8ceab382-16c2-4c1c-bf27-39206ff9a894): /proc/driver/nvidia/version does not exist\n2021-07-19 05:13:10.540191: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2021-07-19 05:13:10.540779: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/e5ccc038-5443-4315-bf00-8b5c4e7685ae","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"3DwOWHAVMfZU","colab_type":"text","cell_id":"ac1776e5dd694db49618421165a7eaee","deepnote_cell_type":"markdown"},"source":"## Compile and Train the Model\n\nSpecify the loss function and optimization routine. Then compile the model you designed. Compiltation of the Keras model initializes the model weights and sets some other model properties.","block_group":"ac1776e5dd694db49618421165a7eaee"},{"cell_type":"code","metadata":{"id":"azIn2h-PMfZV","colab":{},"colab_type":"code","source_hash":"84b70dd8","execution_start":1626671590643,"execution_millis":0,"deepnote_to_be_reexecuted":false,"cell_id":"0f630b309deb4ba892b8b42ce83f133b","deepnote_cell_type":"code"},"source":"# Specify the loss function to use\nloss_func = tf.keras.losses.categorical_crossentropy\n\n# Use the \"Adam\" adaptive learning algorithm to optimize the filter weights\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n\n# Compile the model using the specified loss function and potimizer\nmodel.compile(loss=loss_func, optimizer=opt, metrics=['accuracy'])","block_group":"0f630b309deb4ba892b8b42ce83f133b","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"id":"ffPMiOYRwzgS","colab":{"height":357,"base_uri":"https://localhost:8080/"},"outputId":"130f65ff-4875-4399-c0dd-ec808dcf1b29","colab_type":"code","source_hash":"3ad2535f","execution_start":1626671590643,"execution_millis":27066,"deepnote_to_be_reexecuted":false,"cell_id":"ab6d3576d5d14bc386f5db2f8aabcb2e","deepnote_cell_type":"code"},"source":"# Train Model\n\n# This function is called after each epoch\n# (It will ensure that your training process does not consume all available RAM)\nclass garbage_collect_callback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    gc.collect()\n\nhistory = model.fit(train_images,   # Train examples\n          train_labels,             # Train labels\n          epochs=10,                # number of epochs\n          batch_size= 200,          # number of images for each iteration\n          callbacks=[garbage_collect_callback()],\n          validation_data=(test_images, test_labels), # Data for validation\n          verbose=True)             # Print info about optimization process\n","block_group":"ab6d3576d5d14bc386f5db2f8aabcb2e","execution_count":null,"outputs":[{"name":"stderr","text":"2021-07-19 05:13:10.682535: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n2021-07-19 05:13:10.707355: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2499995000 Hz\nEpoch 1/10\n20/20 [==============================] - 4s 173ms/step - loss: 2.1189 - accuracy: 0.1290 - val_loss: 2.0521 - val_accuracy: 0.1270\nEpoch 2/10\n20/20 [==============================] - 2s 122ms/step - loss: 2.0402 - accuracy: 0.1248 - val_loss: 2.0133 - val_accuracy: 0.1550\nEpoch 3/10\n20/20 [==============================] - 2s 122ms/step - loss: 2.0012 - accuracy: 0.1782 - val_loss: 1.9691 - val_accuracy: 0.1660\nEpoch 4/10\n20/20 [==============================] - 2s 126ms/step - loss: 1.9532 - accuracy: 0.1683 - val_loss: 1.9180 - val_accuracy: 0.1900\nEpoch 5/10\n20/20 [==============================] - 3s 127ms/step - loss: 1.8981 - accuracy: 0.2460 - val_loss: 1.8645 - val_accuracy: 0.3390\nEpoch 6/10\n20/20 [==============================] - 2s 126ms/step - loss: 1.8441 - accuracy: 0.3415 - val_loss: 1.8152 - val_accuracy: 0.3590\nEpoch 7/10\n20/20 [==============================] - 3s 127ms/step - loss: 1.7922 - accuracy: 0.3770 - val_loss: 1.7683 - val_accuracy: 0.3720\nEpoch 8/10\n20/20 [==============================] - 2s 126ms/step - loss: 1.7460 - accuracy: 0.3871 - val_loss: 1.7203 - val_accuracy: 0.3900\nEpoch 9/10\n20/20 [==============================] - 2s 123ms/step - loss: 1.7001 - accuracy: 0.3863 - val_loss: 1.6734 - val_accuracy: 0.3860\nEpoch 10/10\n20/20 [==============================] - 2s 126ms/step - loss: 1.6418 - accuracy: 0.4028 - val_loss: 1.6340 - val_accuracy: 0.4070\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/8a536e3a-8dfd-48a1-8feb-7a33a7b4ee92","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"dF1ufZSGeZ5m","colab_type":"text","cell_id":"c4e7f010b96149f7974268a07c2a501d","deepnote_cell_type":"markdown"},"source":"## Evaluate Model Performance\n\nNow that the model has been trained, we can use it to generate predictions and evaluate its performance.","block_group":"c4e7f010b96149f7974268a07c2a501d"},{"cell_type":"code","metadata":{"id":"_ngFO-dYMfZa","colab":{"height":51,"base_uri":"https://localhost:8080/"},"outputId":"7225d5d7-9c05-4808-ab04-b848ce63f690","colab_type":"code","source_hash":"6b09341f","execution_start":1626671617713,"execution_millis":402,"deepnote_to_be_reexecuted":false,"cell_id":"f71f7e3adf9141728a639bd7384503aa","deepnote_cell_type":"code"},"source":"# Evaluate model using test_images\n# WARNING: the value returned by model.evaluate depends on what metrics were\n# given to the model at compile time. Thus, if the metric is no longer accuracy,\n# the label \"model accuracy\" below will no longer be accurate\n\ntest_binary_pred = model.predict(test_images)\nscores = model.evaluate(test_images, test_labels, verbose=False)\nprint('Testing model on test set:')\nprint(f'Model Loss: {scores[0]:.3f}, Model Accuracy: {scores[1]:.3f}')","block_group":"f71f7e3adf9141728a639bd7384503aa","execution_count":null,"outputs":[{"name":"stdout","text":"Testing model on test set:\nModel Loss: 1.634, Model Accuracy: 0.407\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/e8a4fafd-3c2f-4ce6-aa33-6d5c280324c4","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"sjAqutu2MfZf","colab_type":"text","cell_id":"5ef013d7e1f94a0aa05ad8ae7fef7683","deepnote_cell_type":"markdown"},"source":"Let's take a look at the confusion matrix for these model predictions","block_group":"5ef013d7e1f94a0aa05ad8ae7fef7683"},{"cell_type":"code","metadata":{"id":"yXGdzQ25e7gz","colab":{"height":153,"base_uri":"https://localhost:8080/"},"outputId":"dbc70b2d-fb7c-4c18-9590-d0bc8269289f","colab_type":"code","source_hash":"a30ce339","execution_start":1626671618105,"execution_millis":15,"deepnote_to_be_reexecuted":false,"cell_id":"e45aa6e44b0c42b98ac6f55c0e35007b","deepnote_cell_type":"code"},"source":"# First, convert the one-hot encoded true labels and predictied labels back into\n# a numerical classification value. This can be accomplished with the `argmax`\n# function and that neat `axis` keyword, again.\ntest_true_labels = test_labels.argmax(axis=1)\ntest_pred_labels = test_binary_pred.argmax(axis=1)\n\n# Generate the confusion matrix using these labels\nmat = confusion_matrix(test_true_labels, test_pred_labels)\nprint(mat)","block_group":"e45aa6e44b0c42b98ac6f55c0e35007b","execution_count":null,"outputs":[{"name":"stdout","text":"[[  0  46  88   0   0   0   0   0]\n [  0  90  26   3   0   0  10   0]\n [  0  27  81   1   0   0   4   0]\n [  0  83   7  12   1   0  21   0]\n [  0  78  24   3   0   0  21   0]\n [  0   0   1   0   0 126   0   0]\n [  0  26   0   0   0   0  98   0]\n [  0   0  62   0   0  61   0   0]]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/87c88eca-cc52-4ec0-bbd0-b0865178c7a1","content_dependencies":null},{"cell_type":"markdown","metadata":{"id":"WS3XfFfiftub","colab_type":"text","cell_id":"fcce9547a3db44909a677c7ea390f5ae","deepnote_cell_type":"markdown"},"source":"To get a more visual representation of the matrix, let's plot it in matplotlib.","block_group":"fcce9547a3db44909a677c7ea390f5ae"},{"cell_type":"code","metadata":{"id":"h7RYbDzyMfZm","colab":{"height":580,"base_uri":"https://localhost:8080/"},"outputId":"38166733-3402-4bf0-cd2a-d02b9bbfe622","colab_type":"code","source_hash":"ad47fd3c","execution_start":1626671618116,"execution_millis":267,"deepnote_to_be_reexecuted":false,"cell_id":"9d27860ec474440ea48888eb262c2814","deepnote_cell_type":"code"},"source":"# Generate a new figure\nplt.figure(figsize=(10,10))\n\n# Display the confusion matrix\nplt.imshow(mat, cmap='hot', interpolation='nearest')\n\n# Add some anotation for the plot\nplt.colorbar()\nplt.xlabel('True label')\nplt.ylabel('Predicted label')\nplt.show()","block_group":"9d27860ec474440ea48888eb262c2814","execution_count":null,"outputs":[{"data":{"text/plain":"<Figure size 720x720 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light","image/png":{"width":589,"height":566}},"output_type":"display_data"}],"outputs_reference":"s3:deepnote-cell-outputs-production/df1f1b75-3ec1-450a-a286-76dad1b4ca5f","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=37179657-af6c-4b10-ba18-5b7441fab104' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"de0aaee423d04af487778570accb57bc"}}